{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('FullDayWithAlarms.csv', header=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cod_gioco: Always 8 → probably identifies a single system/machine. to be dropped\n",
    "\n",
    "intervallo_acquisizione: Always 60 seconds → suggests a fixed time granularity (1 minute). to be dropped\n",
    "\n",
    "numero_transazioni: Average ≈ 2246, 204 to 5057 → reflects the intensity of activities in one minute.\n",
    "\n",
    "tempo_min, tempo_max, tempo_medio: \n",
    "- tempo_min has very small values (average ≈ 2.7 sec).\n",
    "\n",
    "- tempo_max has a huge maximum value (92682 sec ≈ 25h!) → obvious outlier. (not to be considered at the moment)\n",
    "\n",
    "- tempo_medio follows a more regular pattern, but also has an outlier maximum (2019 sec).\n",
    "\n",
    "numero_retry: almost always 0 but with a max of 6496 → may indicate errors or temporary technical problems.\n",
    "\n",
    "numero_transazioni_errate: 81 to 448 → possible metric of service quality.\n",
    "\n",
    "\n",
    "in addition to cod game and interval which we drop as constant, we will also drop time min and time max as among the 3 time measures we prefer to keep average time as there is a single unique observation for each minute of the original dataset.\n",
    "time min and max in addition to being unclear and not very representative, we remove them after the data augmentation because they are useful for clipping the average time, once the data augmentation is over we drop time min and max so that we are left only with the 4 features we are interested in: number of transactions, average time, number of retries and number of wrong transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.nunique())\n",
    "\n",
    "for colonna in df.columns:\n",
    "    valori_unici = df[colonna].unique()\n",
    "    print(f\"Valori univoci nella colonna '{colonna}':\")\n",
    "    print(valori_unici)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Rename columns\n",
    "df = df.rename(columns={\n",
    "    'DATA ORA': 'data_ora',\n",
    "    'INTERVALLO\\nACQUISIZIONE': 'intervallo',\n",
    "    'NUMERO\\nTRANSAZIONI': 'numero_transazioni',\n",
    "    'TEMPO MIN': 'tempo_min',\n",
    "    'TEMPO MAX': 'tempo_max',\n",
    "    'TEMPO MEDIO': 'tempo_medio',\n",
    "    'NUMERO RETRY': 'numero_retry',\n",
    "    'NUMERO \\nTRANSAZIONI ERRATE': 'numero_transazioni_errate'\n",
    "})\n",
    "\n",
    "# 2. Remove costant columns\n",
    "df = df.drop(columns=['COD \\nGIOCO', 'intervallo'])\n",
    "\n",
    "# 3. Conversion in datetime \n",
    "df['data_ora'] = pd.to_datetime(df['data_ora'], dayfirst=True)\n",
    "\n",
    "\n",
    "int_cols = [\n",
    "    'numero_transazioni', 'numero_retry', 'numero_transazioni_errate']\n",
    "df[int_cols] = df[int_cols].astype(int)\n",
    "df['tempo_medio'] = df['tempo_medio'].astype(float)\n",
    "\n",
    "# . Controllo rapido\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['data_ora'].min(), df['data_ora'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# 1. Figure & axis with white background\n",
    "# -------------------------------------------------\n",
    "fig, ax = plt.subplots(figsize=(14, 6), facecolor='white')\n",
    "ax.set_facecolor('white')               # axis background\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2. Plot the time-series\n",
    "# -------------------------------------------------\n",
    "ax.plot(df['data_ora'], df['numero_transazioni'], label='numero_transazioni', color='green')\n",
    "ax.plot(df['data_ora'], df['numero_retry'], label='numero_retry', color='orange')\n",
    "ax.plot(df['data_ora'], df['numero_transazioni_errate'], label='numero_transazioni_errate', color='red')\n",
    "ax.plot(df['data_ora'], df['tempo_medio'], label='tempo_medio', color='blue')\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3. Design\n",
    "# -------------------------------------------------\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(interval=1))\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "fig.autofmt_xdate(rotation=45)\n",
    "ax.set_xlabel('Date / Time')\n",
    "ax.set_ylabel('Count / Time')\n",
    "ax.set_title('Valid, retry and errored transactions over time')\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, linestyle='--', linewidth=0.5, color='grey', alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"day_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================================================\n",
    "#  DATA-AUG 07:00 ➜ 02:59  (pausa 03:00-06:59)\n",
    "# ========================================================\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 1) MINUTE-BY-MINUTE STATISTICS  (dtype → float)\n",
    "# --------------------------------------------------------\n",
    "def build_minute_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['min_of_day'] = (\n",
    "        (df['data_ora'] - df['data_ora'].dt.normalize())\n",
    "        .dt.total_seconds() // 60\n",
    "    ).astype(int)\n",
    "\n",
    "    stats = (\n",
    "        df.groupby('min_of_day')\n",
    "          .agg(['min', 'max', 'mean', 'std'])\n",
    "    )\n",
    "    stats.columns = ['_'.join(col) for col in stats.columns]\n",
    "\n",
    "    # --- extend 00:00-02:59 copying 22:00-23:59 ------------\n",
    "    base_block = stats.loc[1380:1439]                     # 23:00-23:59\n",
    "    night_idx  = range(0, 180)\n",
    "    reps       = np.tile(base_block.to_numpy(), (3, 1))[:180]\n",
    "\n",
    "    stats_night = pd.DataFrame(reps, index=night_idx, columns=stats.columns)\n",
    "    stats_ext   = pd.concat([stats_night, stats.loc[420:]], axis=0).sort_index()\n",
    "\n",
    "    stats_ext = stats_ext.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    return stats_ext\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 2) SYNTHETIC DAY 07-03\n",
    "# --------------------------------------------------------\n",
    "def synth_day(stats: pd.DataFrame,\n",
    "              day_shift: int,\n",
    "              slow: bool = False,\n",
    "              rng:  np.random.Generator = np.random.default_rng()):\n",
    "    scale_rng      = (0.70, 0.85) if slow else (0.90, 1.05)\n",
    "    noise_frac     = 0.005        if slow else 0.010\n",
    "    anomaly_chance = 0.005        if slow else 0.020\n",
    "\n",
    "    SPIKE_RATIO = 0.90\n",
    "    SPIKE_RANGE = (1.10, 1.25)\n",
    "    DROP_RANGE  = (0.85, 0.95)\n",
    "    FLOOR_PCT   = 0.95\n",
    "\n",
    "    cols_int   = ['numero_transazioni','tempo_min','tempo_max',\n",
    "                  'numero_retry','numero_transazioni_errate']\n",
    "    cols_float = ['tempo_medio']\n",
    "\n",
    "    sampled = {}\n",
    "    for c in cols_int + cols_float:\n",
    "        mu    = stats[f'{c}_mean'].to_numpy(float)\n",
    "        sigma = stats[f'{c}_std' ].fillna(0).to_numpy(float)\n",
    "        sigma = np.where(sigma == 0, noise_frac * np.maximum(1, mu), sigma)\n",
    "        base  = rng.normal(mu, sigma)\n",
    "        scale = rng.uniform(*scale_rng, size=len(base))\n",
    "        sampled[c] = base * scale\n",
    "\n",
    "    out           = pd.DataFrame(sampled, index=stats.index)\n",
    "    out[cols_int] = out[cols_int].round().astype(int).clip(lower=0)\n",
    "\n",
    "    # clip time\n",
    "    out['tempo_min']   = out['tempo_min' ].clip(lower=1)\n",
    "    out['tempo_max']   = np.maximum(out['tempo_max'], out['tempo_min'] + 1)\n",
    "    out['tempo_medio'] = out['tempo_medio'].clip(out['tempo_min'], out['tempo_max'])\n",
    "\n",
    "    \n",
    "    base_dt   = stats.attrs['base_dt'].normalize() + pd.Timedelta(days=day_shift)\n",
    "    out['data_ora'] = base_dt + pd.to_timedelta(out.index, unit='m')\n",
    "    early_mask = out.index < 420\n",
    "    out.loc[early_mask, 'data_ora'] += pd.Timedelta(days=1)\n",
    "\n",
    "    # --------------------- ANOMALIES ----------------------\n",
    "    hour = (out.index // 60).to_numpy()\n",
    "\n",
    "    busy_mask  = (11 <= hour) & (hour <= 14) | (17 <= hour) & (hour <= 21)\n",
    "    night_mask = (hour < 3)                                    # 00-02\n",
    "    \n",
    "    scale_factor = np.where(busy_mask, 3.0,\n",
    "                     np.where(night_mask, 0.5, 1.0))\n",
    "    p_anom = np.clip(anomaly_chance * scale_factor, 0, 1)\n",
    "\n",
    "    flag_anom = rng.random(len(out)) < p_anom\n",
    "    is_spike  = rng.random(len(out)) < SPIKE_RATIO\n",
    "\n",
    "    spike_f   = rng.uniform(*SPIKE_RANGE, len(out))\n",
    "    drop_f    = rng.uniform(*DROP_RANGE , len(out))\n",
    "\n",
    "    mask_spike = flag_anom &  is_spike\n",
    "    mask_drop  = flag_anom & ~is_spike\n",
    "\n",
    "    out.loc[mask_spike, 'numero_transazioni'] = (\n",
    "        out.loc[mask_spike, 'numero_transazioni'] * spike_f[mask_spike]\n",
    "    ).round().astype(int)\n",
    "\n",
    "    floor = (stats['numero_transazioni_min'] * FLOOR_PCT).to_numpy()\n",
    "    new_vals = (out.loc[mask_drop, 'numero_transazioni'] * drop_f[mask_drop]).round()\n",
    "    out.loc[mask_drop, 'numero_transazioni'] = (\n",
    "        np.maximum(new_vals, floor[mask_drop]).astype(int)\n",
    "    )\n",
    "\n",
    "    out['is_anomaly'] = flag_anom.astype(int)\n",
    "    out = out[(out.index < 180) | (out.index >= 420)].reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# 3) WEEK MAKER\n",
    "# --------------------------------------------------------\n",
    "def make_week(df_original: pd.DataFrame,\n",
    "              rng: np.random.Generator = np.random.default_rng(42)) -> pd.DataFrame:\n",
    "    stats = build_minute_stats(df_original)\n",
    "    stats.attrs['base_dt'] = (\n",
    "        df_original['data_ora'].dt.normalize().iloc[0] + pd.Timedelta(hours=7)\n",
    "    )\n",
    "\n",
    "    week = [df_original.assign(is_anomaly=0)]\n",
    "    for d in range(1, 7):\n",
    "        week.append(synth_day(stats, d, slow=(d == 3), rng=rng))\n",
    "\n",
    "    return (pd.concat(week, ignore_index=True)\n",
    "              .sort_values('data_ora')\n",
    "              .reset_index(drop=True))\n",
    "\n",
    "\n",
    "df_week  = make_week(df)\n",
    "\n",
    "print(f\"Dataset finale: {df_week.shape[0]:,} righe \"\n",
    "      f\"(1 reale + 6 sintetiche) – dal {df_week['data_ora'].min()} \"\n",
    "      f\"al {df_week['data_ora'].max()}\")\n",
    "\n",
    "# esempio rapido di verifica tipi\n",
    "assert df_week['tempo_medio'].dtype == float\n",
    "assert all(df_week[c].dtype == int for c in\n",
    "           ['numero_transazioni','tempo_min','tempo_max',\n",
    "            'numero_retry','numero_transazioni_errate'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knobs to Intervene on for Modifying Data Augmentation\n",
    "\n",
    "| Parameter | Location | Qualitative Effect | Typical Range |\n",
    "|-----------|---------------|---------------------|--------------|\n",
    "| **`scale_rng = (low, high)`** | `synth_day()` | *Multiplicative factor* common to **all** columns.<br>Shifts the entire distribution upwards ( > 1 ) or downwards ( < 1 ). | 0.6 – 1.4 |\n",
    "| **`noise_frac`** | `synth_day()` | Amplitude of **additive** Gaussian noise (σ ≈ `noise_frac · real_std`).<br>Increases minute-by-minute \"graininess\". | 0 – 0.05 |\n",
    "| **`anomaly_chance`** | `synth_day()` | % of records that become anomalies (spike/drop). | 0 – 0.10 |\n",
    "| **`scale_factor`** (within `synth_day`, replacing `peak_boost`) | `synth_day()` inside the anomaly generation block | Multiplier for `anomaly_chance` during busy hours (e.g., 11-14 / 17-21) and potentially a reducer for off-peak hours. <br> *Original logic used `peak_boost = 1.3` as a fixed multiplier. The provided code uses a dynamic `scale_factor`.* | e.g., 0.5 (night), 1.0 (normal), 3.0 (busy) |\n",
    "| **`SPIKE_RATIO`** | `synth_day()` | Probability that an anomaly is a spike (vs. a drop). | 0 – 1 |\n",
    "| **`SPIKE_RANGE = (1.10, 1.25)`**<br>**`DROP_RANGE  = (0.85, 0.95)`** | `synth_day()` | Intensity of spikes or drops. | As desired |\n",
    "| **Sampling Distribution** | Vectorized sampling block in `synth_day()` | Default is **Normal** N(μ, σ).<br>Can be replaced with Uniform, Gamma, etc. | — |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week = df_week.drop(columns=['tempo_min', 'tempo_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_week.to_csv(\"week_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_week.info())\n",
    "print(df_week.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Split original and augmented days\n",
    "df_original_day   = df_week[df_week['data_ora'].dt.date == df_week['data_ora'].dt.date.min()]\n",
    "df_augmented_days = df_week[df_week['data_ora'].dt.date != df_week['data_ora'].dt.date.min()]\n",
    "df_anomalies      = df_week[df_week['is_anomaly'] == 1]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 6))   \n",
    "\n",
    "# 3. Plot original\n",
    "ax.plot(\n",
    "    df_original_day['data_ora'],\n",
    "    df_original_day['numero_transazioni'],\n",
    "    label=\"Original Day\",\n",
    "    linestyle='--',\n",
    "    color='orange',\n",
    "    alpha=0.9\n",
    ")\n",
    "\n",
    "# 4. Plot augmented\n",
    "ax.plot(\n",
    "    df_augmented_days['data_ora'],\n",
    "    df_augmented_days['numero_transazioni'],\n",
    "    label=\"Augmented Days\",\n",
    "    linestyle='--',\n",
    "    color='steelblue',\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# 5. Highlight anomalies (red dots)\n",
    "ax.scatter(\n",
    "    df_anomalies['data_ora'],\n",
    "    df_anomalies['numero_transazioni'],\n",
    "    color='red',\n",
    "    label='Injected Anomalies',\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "ax.set_title(\"numero_transazioni: Original Day vs Augmented Days with Anomalies\")\n",
    "ax.set_xlabel(\"Ora\")\n",
    "ax.set_ylabel(\"Transactions\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "\n",
    "# 7.  ⌚︎ Formatter solo HH:MM  (e locator ogni 2 h, opz.)\n",
    "ax.xaxis.set_major_locator(mdates.HourLocator(interval=3))      # mostra 07:00, 10:00, …\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "\n",
    "plt.setp(ax.get_xticklabels(), rotation=60, ha='right')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df_w = pd.read_csv('week_dataset.csv')\n",
    "df_original_raw = pd.read_csv('day_dataset.csv')\n",
    "\n",
    "# Copy for safety\n",
    "df_processed = df_w.copy()\n",
    "\n",
    "df_processed['data_ora'] = pd.to_datetime(df_processed['data_ora'])\n",
    "\n",
    "print(\"\\n--- General Information about DataFrame df_week ---\")\n",
    "df_processed.info()\n",
    "\n",
    "print(\"\\n--- Descriptive Statistics ---\")\n",
    "print(df_processed.describe().T)\n",
    "\n",
    "print(\"\\n--- Visualization of Numerical Feature Distributions ---\")\n",
    "numerical_cols_to_plot = ['numero_transazioni', 'tempo_medio', 'numero_retry', 'numero_transazioni_errate']\n",
    "\n",
    "for col in numerical_cols_to_plot:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Histogram of the augmented dataset (df_processed)\n",
    "    sns.histplot(df_processed[col], kde=True, bins=50, color=\"blue\", label='Augmented Data (df_week)', stat=\"density\")\n",
    "    \n",
    "    # Overlay histogram of the original dataset (df_original_raw) if available\n",
    "    if df_original_raw is not None and col in df_original_raw.columns:\n",
    "        sns.histplot(df_original_raw[col], kde=True, bins=50, color=\"lightyellow\", label='Original Day Data', stat=\"density\", alpha=0.7)\n",
    "        # Alternative for a more \"background\" visualization for the original:\n",
    "        # sns.kdeplot(df_original_raw[col], color=\"orange\", label='Original Day KDE', fill=True, alpha=0.3, linewidth=0)\n",
    "\n",
    "\n",
    "    plt.title(f'Distribution of {col}: Original vs. Augmented')\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel('Density') # Changed to Density to better compare different shapes\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplots remain useful to see the dispersion and outliers of df_processed\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    sns.boxplot(x=df_processed[col], color=\"blue\", boxprops=dict(edgecolor='black', linewidth=1.5),  # Box properties\n",
    "        whiskerprops=dict(color='green', linestyle='--', linewidth=1.5),  # Whisker properties\n",
    "        medianprops=dict(color='red', linewidth=2),  # Median properties\n",
    "        capprops=dict(color='purple', linewidth=1.5),  # Cap properties\n",
    "        flierprops=dict(marker='o', markerfacecolor='orange', markersize=8, markeredgecolor='gray')  # Outlier properties\n",
    "    )\n",
    "    plt.title(f'Boxplot of {col} (Augmented Data)')\n",
    "    plt.xlabel(col)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(12, 3))\n",
    "    sns.boxplot(x=df_original_raw[col], color=\"lightyellow\", boxprops=dict(edgecolor='black', linewidth=1.5),  # Box properties\n",
    "        whiskerprops=dict(color='green', linestyle='--', linewidth=1.5),  # Whisker properties\n",
    "        medianprops=dict(color='red', linewidth=2),  # Median properties\n",
    "        capprops=dict(color='purple', linewidth=1.5),  # Cap properties\n",
    "        flierprops=dict(marker='o', markerfacecolor='orange', markersize=8, markeredgecolor='gray')  # Outlier properties\n",
    "    )\n",
    "    plt.title(f'Boxplot of {col}') # Kept original title as it refers to df_original_raw\n",
    "    plt.xlabel(col)\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n--- Analysis of Propagated Outliers (Focus on tempo_max and tempo_medio) ---\")\n",
    "# The augmentation code might have limited extreme outliers,\n",
    "# but let's check the maximum generated values.\n",
    "print(\"Maximum values per column in week_dataset:\")\n",
    "print(df_processed[numerical_cols_to_plot].max())\n",
    "print(\"\\nMinimum values per column in week_dataset:\")\n",
    "print(df_processed[numerical_cols_to_plot].min())\n",
    "# Compare these with the maximums/minimums of the original df if necessary.\n",
    "# The `synth_day` function has clipping logic, which should have helped.\n",
    "print(\"\\nMaximum values per column in day_dataset:\")\n",
    "print(df_original_raw[numerical_cols_to_plot].max())\n",
    "print(\"\\nMinimum values per column in day_dataset:\")\n",
    "print(df_original_raw[numerical_cols_to_plot].min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Correlations between Selected Features ---\")\n",
    "selected_features_for_corr = ['numero_transazioni', 'tempo_medio', 'numero_retry', 'numero_transazioni_errate']\n",
    "correlation_matrix = df_processed[selected_features_for_corr].corr()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Selected Feature Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run in the terminal -> mv week_dataset.csv data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell: Library Imports\n",
    "\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Tuple, Dict, Any, List \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import (\n",
    "    LSTM, Dense, Input, Dropout, Conv1D, BatchNormalization, \n",
    "    Add, Activation, GlobalAveragePooling1D, RepeatVector, \n",
    "    TimeDistributed, Bidirectional\n",
    ")\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, \n",
    "    LearningRateScheduler, TensorBoard\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix, roc_curve, auc, \n",
    "    precision_recall_curve, average_precision_score,\n",
    "    precision_score, recall_score, f1_score, \n",
    "    classification_report, roc_auc_score,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "\n",
    "import joblib\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook Setup and Configuration\n",
    "\n",
    "This notebook implements an anomaly detection system for time series data. \n",
    "The first step is to define all configuration parameters and ensure necessary directories for data, trained models, and results are created.\n",
    "\n",
    "**What this cell does:**\n",
    "*   Imports `os` and `pathlib.Path` for path manipulation.\n",
    "*   Defines `PROJECT_ROOT` as the current working directory of the notebook.\n",
    "*   Sets up paths for `DATA_DIR`, `MODELS_DIR`, and `RESULTS_DIR` relative to `PROJECT_ROOT`.\n",
    "*   Creates these directories if they do not already exist, ensuring the project has a standardized structure for storing artifacts.\n",
    "*   Defines crucial data parameters:\n",
    "    *   `DATA_FILE`: The name of the augmented dataset file (`week_dataset.csv`).\n",
    "    *   `TARGET_COLUMN`: The name of the column indicating anomalies (`is_anomaly`).\n",
    "    *   `FEATURE_COLUMNS`: A list of feature names used for model training and analysis.\n",
    "    *   `PREDICTION_FEATURES`: A subset of `FEATURE_COLUMNS` specifically targeted by forecasting models.\n",
    "*   Specifies time series parameters:\n",
    "    *   `SEQUENCE_LENGTH`: The number of past time steps (minutes) to use as input for sequence models (e.g., 60 minutes).\n",
    "    *   `FORECAST_HORIZON`: The number of future time steps (minutes) to predict (e.g., 30 minutes).\n",
    "*   Defines hyperparameters and settings for various models:\n",
    "    *   `LSTM_PARAMS`: Parameters for the LSTM forecasting model, including units, dropout, batch size, epochs, etc. *Epochs are currently set to 1 for quick demonstration runs.*\n",
    "    *   `TCN_PARAMS`: Parameters for the TCN forecasting model. *Epochs are currently set to 1.*\n",
    "    *   `AUTOENCODER_PARAMS`: Parameters for the Autoencoder anomaly detection model, including layer units, dropout, and thresholding strategy. *Epochs are currently set to 1.*\n",
    "*   Sets training data split sizes (`TRAIN_SIZE`, `VAL_SIZE`, `TEST_SIZE`) in terms of number of \"days\" from the augmented dataset.\n",
    "*   Defines evaluation metrics to be used for forecasting and anomaly detection.\n",
    "*   Configures visualization parameters (`PLOT_PARAMS`) for consistent styling of plots generated by `matplotlib` and `seaborn`.\n",
    "*   Sets up print settings (`PRINT_PARAMS`) to control the verbosity of output during script execution (e.g., showing model summaries, timing).\n",
    "*   Defines TensorBoard parameters (`TENSORBOARD_PARAMS`) for monitoring model training.\n",
    "\n",
    "**Why these choices were made:**\n",
    "*   Centralizing configuration makes the project easier to manage, modify, and experiment with.\n",
    "*   Defining paths ensures that data, models, and results are stored in a consistent and predictable manner.\n",
    "*   Specifying model hyperparameters allows for easy tuning and reproducibility of model training.\n",
    "*   Setting epochs to 1 is for rapid execution during development or demonstration; for actual training, these would be higher (e.g., 50 as in the original scripts).\n",
    "*   Standardized plotting and print parameters ensure consistent output across the notebook.\n",
    "\n",
    "**Expected Output:**\n",
    "This cell will print messages confirming that the configuration parameters have been loaded and that the necessary project directories (`data`, `models`, `results`) have been created or already exist. It will also print the absolute paths to these directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Configuration Parameters\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "PROJECT_ROOT = Path.cwd() \n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
    "\n",
    "\n",
    "for dir_path in [DATA_DIR, MODELS_DIR, RESULTS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True) \n",
    "\n",
    "# Data parameters\n",
    "DATA_FILE = \"week_dataset.csv\" \n",
    "TARGET_COLUMN = \"is_anomaly\"\n",
    "FEATURE_COLUMNS = [\n",
    "    \"numero_transazioni\",\n",
    "    \"tempo_medio\",\n",
    "    \"numero_retry\",\n",
    "    \"numero_transazioni_errate\"\n",
    "]\n",
    "\n",
    "# Prediction features (subset of FEATURE_COLUMNS)\n",
    "PREDICTION_FEATURES = [\n",
    "    \"numero_transazioni\",\n",
    "    \"numero_transazioni_errate\"\n",
    "]\n",
    "\n",
    "# Time series parameters\n",
    "SEQUENCE_LENGTH = 60  # Number of time steps to look back (60 minutes)\n",
    "FORECAST_HORIZON = 30  # Number of time steps to predict ahead (30 minutes)\n",
    "\n",
    "# Model parameters\n",
    "LSTM_PARAMS = {\n",
    "    \"units\": 64,  \n",
    "    \"dropout\": 0.3,  \n",
    "    \"recurrent_dropout\": 0.2, \n",
    "    \"batch_size\": 64,  \n",
    "    \"epochs\": 50,  \n",
    "    \"patience\": 10,  \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"epsilon\": 1e-07\n",
    "}\n",
    "\n",
    "TCN_PARAMS = {\n",
    "    \"nb_filters\": 64,  \n",
    "    \"kernel_size\": 4,\n",
    "    \"nb_stacks\": 3,  \n",
    "    \"dilations\": [1, 2, 4, 8],  \n",
    "    \"batch_size\": 64,  \n",
    "    \"epochs\": 50,  \n",
    "    \"patience\": 10,  \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"epsilon\": 1e-07\n",
    "}\n",
    "\n",
    "AUTOENCODER_PARAMS = {\n",
    "    \"units\": [64, 32, 16], \n",
    "    \"dropout\": 0.2,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 50,\n",
    "    \"patience\": 3, \n",
    "    \"learning_rate\": 0.001,\n",
    "    \"beta_1\": 0.9,\n",
    "    \"beta_2\": 0.999,\n",
    "    \"epsilon\": 1e-07,\n",
    "    \"reconstruction_threshold_percentile\": 95,\n",
    "    \"forecast_horizon\": 30, \n",
    "    \"threshold_analysis_percentiles\": [75, 80, 85, 90, 92, 95, 97, 98, 99, 99.5],\n",
    "    \"threshold_optimization_metric\": \"f1\"  \n",
    "}\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_SIZE = 5 # Number of days for training\n",
    "VAL_SIZE = 1   # Number of days for validation\n",
    "TEST_SIZE = 1  # Number of days for testing\n",
    "\n",
    "# Evaluation parameters\n",
    "ANOMALY_THRESHOLD = 0.95  \n",
    "METRICS = {\n",
    "    \"forecasting\": [\"mse\", \"mae\", \"rmse\"],\n",
    "    \"anomaly_detection\": [\"precision\", \"recall\", \"f1\", \"auc_roc\"]\n",
    "}\n",
    "\n",
    "# Visualization parameters\n",
    "PLOT_PARAMS = {\n",
    "    \"figsize\": (12, 8),\n",
    "    \"dpi\": 100,\n",
    "    \"rc\": {\n",
    "        \"figure.figsize\": (12, 8), \n",
    "        \"figure.dpi\": 100,        \n",
    "        \"font.size\": 12,\n",
    "        \"axes.titlesize\": 14,\n",
    "        \"axes.labelsize\": 12,\n",
    "        \"xtick.labelsize\": 10,\n",
    "        \"ytick.labelsize\": 10,\n",
    "        \"legend.fontsize\": 10,\n",
    "        \"lines.linewidth\": 2,\n",
    "        \"lines.markersize\": 6,\n",
    "        \"grid.alpha\": 0.3\n",
    "    },\n",
    "    \"context\": \"notebook\", \n",
    "    \"palette\": \"deep\"      \n",
    "}\n",
    "\n",
    "# Print settings\n",
    "PRINT_PARAMS = {\n",
    "    \"show_summary\": True,\n",
    "    \"show_progress\": True,\n",
    "    \"show_metrics\": True,\n",
    "    \"show_timing\": True,\n",
    "    \"precision\": 4 \n",
    "}\n",
    "\n",
    "# TensorBoard settings\n",
    "TENSORBOARD_PARAMS = {\n",
    "    \"log_dir\": str(RESULTS_DIR / \"logs\"),\n",
    "    \"histogram_freq\": 1,\n",
    "    \"write_graph\": True,\n",
    "    \"write_images\": True,\n",
    "    \"update_freq\": \"epoch\",\n",
    "    \"profile_batch\": \"500,520\" \n",
    "} \n",
    "\n",
    "print(\"Configuration parameters loaded and directories ensured.\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions\n",
    "\n",
    "This cell defines a collection of helper functions that will be used throughout the notebook for various tasks such as data loading, preprocessing, sequence creation, model saving/loading, and plotting.\n",
    "\n",
    "**What this cell does:**\n",
    "*   **`load_and_preprocess_data()`**:\n",
    "    *   Loads the dataset specified by `DATA_FILE` from the `DATA_DIR`.\n",
    "    *   Converts the `data_ora` column to datetime objects.\n",
    "    *   Sets the `data_ora` column as the DataFrame index.\n",
    "    *   *Purpose:* To provide a standardized way to load and perform initial a_ora` column as the DataFrame index.\n",
    "    *   *Purpose:* To provide a standardized way to load and perform initial common preprocessing steps on the raw dataset.\n",
    "*   **`create_sequences(data, seq_length)`**:\n",
    "    *   Takes a NumPy array of time series data and a sequence length.\n",
    "    *   Generates input sequences (X) of `seq_length` and corresponding target values (y) for the next time step.\n",
    "    *   *Purpose:* To transform time series data into a supervised learning format suitable for sequence models like LSTMs and TCNs.\n",
    "*   **`split_data_chronological(df)`**:\n",
    "    *   Splits the input DataFrame chronologically into training, validation, and test sets based on the `TRAIN_SIZE`, `VAL_SIZE`, and `TEST_SIZE` (number of days) defined in the configuration.\n",
    "    *   It identifies unique days in the dataset and partitions the data accordingly.\n",
    "    *   *Purpose:* To ensure a realistic evaluation scenario for time series models by preventing data leakage from future periods into the training or validation sets.\n",
    "*   **`scale_data(train_data, val_data, test_data)`**:\n",
    "    *   Initializes a `StandardScaler` from `sklearn.preprocessing`.\n",
    "    *   Fits the scaler *only* on the `FEATURE_COLUMNS` of the training data.\n",
    "    *   Transforms the training, validation, and test datasets using the fitted scaler.\n",
    "    *   *Purpose:* To standardize the features to have zero mean and unit variance, which often improves the performance and stability of machine learning models, especially neural networks.\n",
    "*   **Plotting Functions (`plot_forecast_results`, `plot_anomaly_scores`, `plot_confusion_matrix`, `plot_roc_curve`, `plot_precision_recall_curve`, `plot_anomaly_scores_distribution`)**:\n",
    "    *   These functions provide standardized ways to visualize model performance for forecasting and anomaly detection tasks. They generate and save plots like actual vs. predicted, anomaly score distributions, confusion matrices, ROC curves, and Precision-Recall curves to the `RESULTS_DIR`.\n",
    "    *   *Purpose:* To offer visual insights into model behavior and evaluation metrics.\n",
    "*   **`calculate_metrics(y_true, y_pred)`**:\n",
    "    *   Calculates common classification metrics (precision, recall, F1-score, AUC-ROC) given true and predicted binary labels. Handles potential `ValueError` for AUC if only one class is present.\n",
    "    *   *Purpose:* To provide quantitative measures of anomaly detection performance. (Note: The original `utils.py` version was more geared towards classification; forecasting metrics are typically MSE, MAE, RMSE, calculated directly in their respective evaluation functions).\n",
    "*   **`save_model(model, model_name)`**:\n",
    "    *   Saves a trained model (Keras model or other types via `joblib`) to the `MODELS_DIR`.\n",
    "    *   *Purpose:* To persist trained models for later use or deployment.\n",
    "*   **`load_model(model_name)`**:\n",
    "    *   Loads a previously saved model from the `MODELS_DIR`.\n",
    "    *   *Purpose:* To retrieve trained models without needing to retrain them.\n",
    "*   **`plot_anomaly_results(...)`**: A comprehensive plotting function specifically for anomaly detection, combining several visualizations. *This was present in the original `utils.py` and is slightly different from the individual plotting functions that were later moved into Cell 5. For a single notebook, these would be consolidated.*\n",
    "\n",
    "**Why these functions were chosen/created:**\n",
    "*   Encapsulating repetitive tasks into functions makes the main workflow cleaner, more readable, and less prone to errors.\n",
    "*   Standardized data loading, splitting, and scaling ensure consistency across different modeling stages.\n",
    "*   Reusable plotting functions allow for consistent visualization of results.\n",
    "\n",
    "**Expected Output:**\n",
    "This cell primarily defines functions. When executed, it will print \"Utility functions defined.\" to confirm that the functions are now available in the notebook's namespace for subsequent cells to use. No other visual output is expected from this cell itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Utility Functions\n",
    "\n",
    "def load_and_preprocess_data() -> pd.DataFrame:\n",
    "    \"\"\"Load and preprocess the dataset.\"\"\"\n",
    "    \n",
    "    df_path = DATA_DIR / DATA_FILE\n",
    "    if not df_path.exists():\n",
    "        raise FileNotFoundError(f\"Data file not found: {df_path}. Please ensure '{DATA_FILE}' is in the '{DATA_DIR}' directory.\")\n",
    "    df = pd.read_csv(df_path)\n",
    "    df['data_ora'] = pd.to_datetime(df['data_ora'])\n",
    "    df.set_index('data_ora', inplace=True)\n",
    "    return df\n",
    "\n",
    "def create_sequences(data: np.ndarray, seq_length: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Create sequences for time series forecasting.\"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:(i + seq_length)])\n",
    "        y.append(data[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def split_data_chronological(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split data chronologically into train, validation, and test sets.\"\"\"\n",
    "    \n",
    "    total_days_in_data = (df.index.max() - df.index.min()).days + 1 \n",
    "    # This calculation of days_per_split might be problematic if data isn't perfectly contiguous or doesn't start at midnight\n",
    "    # A more robust way if splits are by full days from the start of the dataset:\n",
    "    unique_days = sorted(df.index.normalize().unique())\n",
    "    \n",
    "    if len(unique_days) < (TRAIN_SIZE + VAL_SIZE + TEST_SIZE):\n",
    "        raise ValueError(f\"Not enough unique days in the dataset ({len(unique_days)}) to satisfy the split sizes (Train:{TRAIN_SIZE}, Val:{VAL_SIZE}, Test:{TEST_SIZE}).\")\n",
    "\n",
    "    train_end_date = unique_days[TRAIN_SIZE - 1]\n",
    "    val_end_date = unique_days[TRAIN_SIZE + VAL_SIZE - 1]\n",
    "\n",
    "    train_data = df[df.index.normalize() <= train_end_date]\n",
    "    val_data = df[(df.index.normalize() > train_end_date) & (df.index.normalize() <= val_end_date)]\n",
    "    test_data = df[df.index.normalize() > val_end_date]\n",
    "    \n",
    "    \n",
    "    print(f\"Train data shape: {train_data.shape}, from {train_data.index.min()} to {train_data.index.max()}\")\n",
    "    print(f\"Validation data shape: {val_data.shape}, from {val_data.index.min()} to {val_data.index.max()}\")\n",
    "    print(f\"Test data shape: {test_data.shape}, from {test_data.index.min()} to {test_data.index.max()}\")\n",
    "    \n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "\n",
    "def scale_data(train_data: pd.DataFrame, val_data: pd.DataFrame, test_data: pd.DataFrame) -> Tuple[StandardScaler, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"Scale the data using StandardScaler.\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train_data[FEATURE_COLUMNS])\n",
    "    val_scaled = scaler.transform(val_data[FEATURE_COLUMNS])\n",
    "    test_scaled = scaler.transform(test_data[FEATURE_COLUMNS])\n",
    "    return scaler, train_scaled, val_scaled, test_scaled\n",
    "\n",
    "def plot_forecast_results(actual: np.ndarray, predicted: np.ndarray, title: str, feature_names: List[str]):\n",
    "    \"\"\"Plot actual vs predicted values for each feature.\"\"\"\n",
    "    \n",
    "    n_features = len(feature_names)\n",
    "    if n_features == 0:\n",
    "        print(\"No features to plot for forecast results.\")\n",
    "        return\n",
    "    fig, axes = plt.subplots(n_features, 1, figsize=(12, 4*n_features), squeeze=False) # Ensure axes is always 2D\n",
    "    \n",
    "    for i, (ax, feature) in enumerate(zip(axes.ravel(), feature_names)):\n",
    "        ax.plot(actual[:, i], label='Actual', alpha=0.7)\n",
    "        ax.plot(predicted[:, i], label='Predicted', alpha=0.7)\n",
    "        ax.set_title(f'{feature} - {title}')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'forecast_{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close(fig) \n",
    "\n",
    "def plot_anomaly_scores(scores: np.ndarray, threshold: float, title: str):\n",
    "    \"\"\"Plot anomaly scores with threshold.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(scores, label='Anomaly Score')\n",
    "    plt.axhline(y=threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "    plt.title(f'Anomaly Scores - {title}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(RESULTS_DIR / f'anomaly_scores_{title.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "def calculate_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Calculate evaluation metrics.\"\"\"\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "    \n",
    "    # Ensure y_true and y_pred are binary for these metrics\n",
    "    y_true_binary = (y_true > 0.5).astype(int)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "    # Handling cases where a class might be absent in predictions (e.g., no anomalies predicted)\n",
    "    # or in true labels (though less likely for anomaly detection test sets with anomalies)\n",
    "    # zero_division=0 will return 0.0 if a class is not present in predictions for precision/recall/f1\n",
    "    # For roc_auc_score, it requires at least one sample from each class in y_true for a meaningful score.\n",
    "    # If only one class present in y_true, roc_auc_score is undefined.\n",
    "\n",
    "    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n",
    "    \n",
    "    auc_roc_val = np.nan # Default to NaN if AUC cannot be computed\n",
    "    if len(np.unique(y_true_binary)) > 1: # Check if more than one class in true labels\n",
    "        try:\n",
    "            auc_roc_val = roc_auc_score(y_true_binary, y_pred) # y_pred should be scores/probabilities for AUC\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning: Could not calculate AUC ROC: {e}. This often happens if y_pred contains only one class.\")\n",
    "            # Or if y_pred contains class labels instead of probabilities.\n",
    "            # For anomaly detection, y_pred is often binary labels from a threshold, \n",
    "            # y_score (the raw anomaly scores) should be used for roc_auc_score.\n",
    "            # This function might need y_score as an argument if used for anomaly detection AUC.\n",
    "            # For now, assuming y_pred can be scores for some contexts.\n",
    "\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc_roc': auc_roc_val\n",
    "    }\n",
    "\n",
    "def save_model(model: Any, model_name: str):\n",
    "    \"\"\"Save model to disk.\"\"\"\n",
    "    # Assuming MODELS_DIR is defined\n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        model.save(MODELS_DIR / f'{model_name}.keras')\n",
    "    else:\n",
    "        import joblib\n",
    "        joblib.dump(model, MODELS_DIR / f'{model_name}.joblib')\n",
    "    print(f\"Model '{model_name}' saved to {MODELS_DIR}\")\n",
    "\n",
    "\n",
    "def load_model(model_name: str) -> Any:\n",
    "    \"\"\"Load model from disk.\"\"\"\n",
    "    # Assuming MODELS_DIR is defined\n",
    "    model_path_keras = MODELS_DIR / f'{model_name}.keras'\n",
    "    model_path_joblib = MODELS_DIR / f'{model_name}.joblib'\n",
    "\n",
    "    if model_path_keras.exists():\n",
    "        print(f\"Loading Keras model: {model_path_keras}\")\n",
    "        return tf.keras.models.load_model(model_path_keras)\n",
    "    elif model_path_joblib.exists():\n",
    "        import joblib\n",
    "        print(f\"Loading Joblib model: {model_path_joblib}\")\n",
    "        return joblib.load(model_path_joblib)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found for '{model_name}' in {MODELS_DIR}\")\n",
    "\n",
    "\n",
    "def plot_anomaly_results(y_true: np.ndarray, \n",
    "                        y_pred: np.ndarray, \n",
    "                        anomaly_scores: np.ndarray,\n",
    "                        threshold: float,\n",
    "                        model_name: str,\n",
    "                        save_path: Path) -> None:\n",
    "    \"\"\"Plot anomaly detection results.\n",
    "    \n",
    "    Args:\n",
    "        y_true: True labels (0 for normal, 1 for anomaly)\n",
    "        y_pred: Predicted labels (binary)\n",
    "        anomaly_scores: Anomaly scores for each sample\n",
    "        threshold: Anomaly threshold used to derive y_pred\n",
    "        model_name: Name of the model\n",
    "        save_path: Path to save the plots\n",
    "    \"\"\"\n",
    "    # Ensure y_true is binary\n",
    "    y_true_binary = (np.any(y_true > 0, axis=1)).astype(int) if len(y_true.shape) > 1 and y_true.shape[1] > 1 else (y_true > 0.5).astype(int)\n",
    "    save_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Plot 1: Anomaly Score Distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df_scores = pd.DataFrame({\n",
    "        'Score': anomaly_scores,\n",
    "        'Class': ['Anomaly' if y == 1 else 'Normal' for y in y_true_binary]\n",
    "    })\n",
    "    sns.histplot(data=df_scores, x='Score', hue='Class', kde=True, \n",
    "                 palette={'Normal': 'blue', 'Anomaly': 'red'}, alpha=0.6)\n",
    "    plt.axvline(x=threshold, color='r', linestyle='--', label=f'Threshold: {threshold:.4f}')\n",
    "    plt.title(f'{model_name} - Anomaly Scores Distribution')\n",
    "    plt.xlabel('Anomaly Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend()\n",
    "    plt.savefig(save_path / f'{model_name}_Anomaly_Scores_Distribution.png') # Adjusted filename\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 2: Confusion Matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_true_binary, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(save_path / f'{model_name}_Confusion_Matrix.png') # Adjusted filename\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 3: ROC Curve\n",
    "    if len(np.unique(y_true_binary)) > 1: # ROC AUC is meaningful only with multiple classes\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        fpr, tpr, _ = roc_curve(y_true_binary, anomaly_scores) # Use scores for ROC\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'{model_name} - ROC Curve')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(save_path / f'{model_name}_ROC_Curve.png') # Adjusted filename\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"Skipping ROC curve for {model_name} as y_true_binary contains only one class.\")\n",
    "\n",
    "    # Plot 4: Precision-Recall Curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    precision_curve, recall_curve, _ = precision_recall_curve(y_true_binary, anomaly_scores) # Use scores for PR curve\n",
    "    avg_precision = average_precision_score(y_true_binary, anomaly_scores)\n",
    "    plt.plot(recall_curve, precision_curve, label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{model_name} - Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(save_path / f'{model_name}_Precision_Recall_Curve.png') # Adjusted filename\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot 5: Threshold Analysis (if CSV exists)\n",
    "    threshold_csv_path = RESULTS_DIR / 'autoencoder_threshold_analysis.csv' # Assuming it's in RESULTS_DIR\n",
    "    if threshold_csv_path.exists(): # Check if RESULTS_DIR is defined\n",
    "        threshold_df = pd.read_csv(threshold_csv_path)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['Precision'], label='Precision')\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['Recall'], label='Recall')\n",
    "        plt.plot(threshold_df['Threshold'], threshold_df['F1'], label='F1 Score')\n",
    "        # Highlight the operational threshold used for y_pred\n",
    "        plt.axvline(x=threshold, color='r', linestyle='--', label=f'Chosen Threshold: {threshold:.4f}')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title(f'{model_name} - Threshold Analysis (from Validation)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.savefig(save_path / f'{model_name}_Threshold_Analysis.png') # Adjusted filename\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(f\"Threshold analysis CSV not found at {threshold_csv_path}, skipping plot.\")\n",
    "\n",
    "print(\"Utility functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation Pipeline\n",
    "\n",
    "This cell executes the main data preparation pipeline. It leverages the utility functions defined previously to load the augmented dataset, perform chronological splitting, scale the features, and create sequences suitable for both time series forecasting and anomaly detection models.\n",
    "\n",
    "**What this cell does:**\n",
    "1.  **Load and Preprocess:** Calls `load_and_preprocess_data()` to load `week_dataset.csv` and set the datetime index.\n",
    "2.  **Chronological Split:** Calls `split_data_chronological()` to divide the dataset into training (5 days), validation (1 day), and test (1 day) sets.\n",
    "3.  **Save Test Timestamps:** Saves the timestamps corresponding to the test set sequences. This is useful for aligning predictions with actual times during evaluation and plotting.\n",
    "4.  **Forecasting Data Preparation:**\n",
    "    *   Filters the training and validation sets to include *only normal data* (where `is_anomaly == 0`) for training the forecasting models. This ensures the models learn to predict normal behavior.\n",
    "    *   Initializes and fits a `StandardScaler` (`scaler_forecast`) *only* on the features of the normal training data.\n",
    "    *   Transforms the normal training, normal validation, and the full (mixed normal/anomaly) test data using this scaler.\n",
    "    *   Saves `scaler_forecast.joblib`.\n",
    "    *   Calls `create_sequences()` to generate input sequences (X) and target values (y) for LSTM/TCN models from the scaled forecasting data.\n",
    "    *   Saves the `X_train_forecast`, `y_train_forecast`, `X_val_forecast`, `y_val_forecast`, `X_test_forecast`, and `y_test_forecast` arrays as `.npy` files in the `DATA_DIR`.\n",
    "5.  **Anomaly Detection Data Preparation (for Autoencoder):**\n",
    "    *   Filters the training set to include *only normal data* for training the autoencoder.\n",
    "    *   Initializes and fits a separate `StandardScaler` (`scaler_ad`) *only* on the features of this normal training data.\n",
    "    *   Transforms the normal training data, and the full (mixed normal/anomaly) validation and test data using this `scaler_ad`.\n",
    "    *   Saves `scaler_ad.joblib`.\n",
    "    *   Calls `create_sequences()` to generate input sequences for the autoencoder:\n",
    "        *   `X_train_ad`: Sequences from scaled normal training data (target is to reconstruct these).\n",
    "        *   `X_val_ad_mixed`, `X_test_ad_mixed`: Sequences from scaled mixed validation and test data.\n",
    "        *   `y_val_ad_labels`, `y_test_ad_labels`: The corresponding true anomaly labels for the validation and test sequences.\n",
    "    *   Saves these arrays as `.npy` files.\n",
    "6.  **Save Split DataFrames:** Saves the raw (unscaled, unsequenced) `train_data`, `val_data`, and `test_data` DataFrames to CSV files for reference or other analyses.\n",
    "7.  **Create Basic Visualizations:** Calls `create_visualizations()` to generate and save plots showing feature distributions, time series splits, correlation matrix of training data, and anomaly distribution across splits. These plots are saved in `results/visualizations_prepare_data/`.\n",
    "\n",
    "**Why these steps are performed:**\n",
    "*   **Separate Scalers:** Using distinct scalers for forecasting and anomaly detection, both fitted only on normal training data, is crucial. Forecasting models learn normal patterns, and the autoencoder must learn to reconstruct normal data with low error.\n",
    "*   **Sequence Creation:** Time series models like LSTM, TCN, and sequence-based Autoencoders require input data to be structured as sequences of past observations.\n",
    "*   **Saving Intermediate Data:** Saving processed data arrays and scalers allows for resuming experiments or using these preprocessed components in other parts of the project without rerunning the entire preparation pipeline.\n",
    "*   **Initial Visualizations:** Help in understanding the data characteristics after splitting and identifying any potential issues before model training.\n",
    "\n",
    "**Expected Output:**\n",
    "This cell will print several messages indicating the progress of data loading, splitting, scaling, and sequence creation. It will also print:\n",
    "*   Shapes of the train, validation, and test DataFrames.\n",
    "*   Confirmation messages when scalers and data arrays are saved.\n",
    "*   A final message \"Data preparation completed successfully!\".\n",
    "*   Several plot files will be saved to the `results/visualizations_prepare_data/` directory (e.g., `split_feature_distributions.png`, `split_feature_timeseries.png`, etc.).\n",
    "The function will return a dictionary `prepared_data_dict` containing all the processed DataFrames and NumPy arrays, which can be used by subsequent cells if needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase0 data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Data Preparation Script Logic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# Assuming config variables are defined in Cell 1 (config.py content)\n",
    "# Assuming utility functions are defined in Cell 2 (utils.py content)\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Main function to prepare the dataset for the project.\"\"\"\n",
    "    print(\"Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess_data() # from utils\n",
    "    \n",
    "    # Create time-based features\n",
    "    # df['hour'] = df.index.hour # This was in your original script, but if FEATURE_COLUMNS does not include them,\n",
    "    # df['day_of_week'] = df.index.dayofweek # they won't be scaled or used by models unless explicitly added to FEATURE_COLUMNS.\n",
    "                                          # For now, assuming FEATURE_COLUMNS are the primary ones.\n",
    "\n",
    "    # Split data chronologically\n",
    "    print(\"Splitting data into train, validation, and test sets...\")\n",
    "    train_data, val_data, test_data = split_data_chronological(df) # from utils\n",
    "    \n",
    "    # Save timestamps for later use (original full dataframe index)\n",
    "    # This might be better if timestamps are saved for train, val, test separately or handled during evaluation.\n",
    "    # For simplicity in a notebook, ensure `df.index` corresponds to the full dataset before splitting if that's the intent.\n",
    "    # Or, save timestamps from test_data for final evaluation plotting.\n",
    "    # Let's save the index of the test_data's sequences for precise plotting later.\n",
    "    # Timestamps for X_test sequences will be (len(test_data) - SEQUENCE_LENGTH)\n",
    "    test_timestamps_for_sequences = test_data.index[SEQUENCE_LENGTH:]\n",
    "    np.save(DATA_DIR / 'test_timestamps_for_sequences.npy', test_timestamps_for_sequences.values)\n",
    "    \n",
    "    # Prepare data for forecasting (using only normal data for training)\n",
    "    print(\"Preparing data for forecasting...\")\n",
    "    train_data_forecast_normal_only = train_data[train_data[TARGET_COLUMN] == 0].copy()\n",
    "    val_data_forecast_normal_only = val_data[val_data[TARGET_COLUMN] == 0].copy() # Validation also on normal for forecasting model dev\n",
    "    \n",
    "    # Scale the features for forecasting\n",
    "    scaler_forecast = StandardScaler()\n",
    "    # Fit scaler ONLY on normal training data features\n",
    "    train_scaled_forecast = scaler_forecast.fit_transform(train_data_forecast_normal_only[FEATURE_COLUMNS])\n",
    "    # Transform validation and test data (test data includes anomalies, which is fine for evaluation AFTER model is trained on normal)\n",
    "    val_scaled_forecast = scaler_forecast.transform(val_data_forecast_normal_only[FEATURE_COLUMNS]) # Scale normal validation data\n",
    "    test_scaled_forecast = scaler_forecast.transform(test_data[FEATURE_COLUMNS]) # Scale all test data\n",
    "    \n",
    "    # Save the scaler for forecasting\n",
    "    joblib.dump(scaler_forecast, MODELS_DIR / 'scaler_forecast.joblib')\n",
    "    \n",
    "    # Create sequences for time series forecasting\n",
    "    print(\"Creating sequences for time series forecasting...\")\n",
    "    X_train_forecast, y_train_forecast = create_sequences(train_scaled_forecast, SEQUENCE_LENGTH)\n",
    "    X_val_forecast, y_val_forecast = create_sequences(val_scaled_forecast, SEQUENCE_LENGTH)\n",
    "    X_test_forecast, y_test_forecast = create_sequences(test_scaled_forecast, SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Save forecasting data\n",
    "    np.save(DATA_DIR / 'X_train_forecast.npy', X_train_forecast)\n",
    "    np.save(DATA_DIR / 'y_train_forecast.npy', y_train_forecast)\n",
    "    np.save(DATA_DIR / 'X_val_forecast.npy', X_val_forecast)\n",
    "    np.save(DATA_DIR / 'y_val_forecast.npy', y_val_forecast)\n",
    "    np.save(DATA_DIR / 'X_test_forecast.npy', X_test_forecast)\n",
    "    np.save(DATA_DIR / 'y_test_forecast.npy', y_test_forecast)\n",
    "    \n",
    "    # Prepare data for anomaly detection (Autoencoder: train on normal, validate/test on mixed)\n",
    "    print(\"Preparing data for anomaly detection (Autoencoder)...\")\n",
    "    train_data_ad_normal_only = train_data[train_data[TARGET_COLUMN] == 0].copy()\n",
    "    \n",
    "    # Scale the features for anomaly detection (Autoencoder)\n",
    "    scaler_ad = StandardScaler()\n",
    "    # Fit scaler ONLY on normal training data features\n",
    "    train_scaled_ad_normal_only = scaler_ad.fit_transform(train_data_ad_normal_only[FEATURE_COLUMNS])\n",
    "    # Transform validation and test data (these will contain anomalies)\n",
    "    val_scaled_ad_mixed = scaler_ad.transform(val_data[FEATURE_COLUMNS])\n",
    "    test_scaled_ad_mixed = scaler_ad.transform(test_data[FEATURE_COLUMNS])\n",
    "    \n",
    "    # Save the scaler for anomaly detection\n",
    "    joblib.dump(scaler_ad, MODELS_DIR / 'scaler_ad.joblib')\n",
    "    \n",
    "    # Create sequences for anomaly detection\n",
    "    # Autoencoder trains to reconstruct normal data\n",
    "    X_train_ad_normal_only, _ = create_sequences(train_scaled_ad_normal_only, SEQUENCE_LENGTH) # y_train_ad is X_train_ad for autoencoders\n",
    "    \n",
    "    # For validation and testing, we need X and the original y (anomaly labels)\n",
    "    X_val_ad_mixed, _ = create_sequences(val_scaled_ad_mixed, SEQUENCE_LENGTH) # We'll use the true anomalies from val_data\n",
    "    y_val_ad_labels = val_data[TARGET_COLUMN].values[SEQUENCE_LENGTH:] # Align labels with sequences\n",
    "\n",
    "    X_test_ad_mixed, _ = create_sequences(test_scaled_ad_mixed, SEQUENCE_LENGTH)\n",
    "    y_test_ad_labels = test_data[TARGET_COLUMN].values[SEQUENCE_LENGTH:] # Align labels with sequences\n",
    "    \n",
    "    # Save anomaly detection data\n",
    "    np.save(DATA_DIR / 'X_train_ad.npy', X_train_ad_normal_only) # Train autoencoder only on normal sequences\n",
    "    np.save(DATA_DIR / 'X_val_ad.npy', X_val_ad_mixed)\n",
    "    np.save(DATA_DIR / 'y_val_ad_labels.npy', y_val_ad_labels) # Save actual anomaly labels for validation\n",
    "    np.save(DATA_DIR / 'X_test_ad.npy', X_test_ad_mixed)\n",
    "    np.save(DATA_DIR / 'y_test_ad_labels.npy', y_test_ad_labels) # Save actual anomaly labels for test\n",
    "    \n",
    "    # Save original dataframes (already split) for reference\n",
    "    train_data.to_csv(DATA_DIR / 'train_data.csv')\n",
    "    val_data.to_csv(DATA_DIR / 'val_data.csv')\n",
    "    test_data.to_csv(DATA_DIR / 'test_data.csv')\n",
    "    \n",
    "    \n",
    "    print(\"Creating basic visualizations...\")\n",
    "    create_visualizations(train_data, val_data, test_data) # from utils\n",
    "    \n",
    "    print(\"Data preparation completed successfully!\")\n",
    "    \n",
    "    return { \n",
    "        'train_data': train_data,\n",
    "        'val_data': val_data,\n",
    "        'test_data': test_data,\n",
    "        'X_train_forecast': X_train_forecast,\n",
    "        'y_train_forecast': y_train_forecast,\n",
    "        'X_val_forecast': X_val_forecast,\n",
    "        'y_val_forecast': y_val_forecast,\n",
    "        'X_test_forecast': X_test_forecast,\n",
    "        'y_test_forecast': y_test_forecast,\n",
    "        'X_train_ad': X_train_ad_normal_only,\n",
    "        'X_val_ad': X_val_ad_mixed,\n",
    "        'y_val_ad_labels': y_val_ad_labels,\n",
    "        'X_test_ad': X_test_ad_mixed,\n",
    "        'y_test_ad_labels': y_test_ad_labels\n",
    "    }\n",
    "\n",
    "def create_visualizations(train_data, val_data, test_data):\n",
    "    \"\"\"Create basic visualizations of the data.\"\"\"\n",
    "    # Create directory for visualizations if it doesn't exist\n",
    "    # Assuming RESULTS_DIR and FEATURE_COLUMNS, TARGET_COLUMN are defined\n",
    "    vis_dir = RESULTS_DIR / 'visualizations_prepare_data' # Differentiate from other potential visualizations\n",
    "    vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Plot feature distributions\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(FEATURE_COLUMNS, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        sns.histplot(train_data[feature], label='Train', alpha=0.5, kde=True)\n",
    "        sns.histplot(val_data[feature], label='Validation', alpha=0.5, kde=True)\n",
    "        sns.histplot(test_data[feature], label='Test', alpha=0.5, kde=True)\n",
    "        plt.title(f'{feature} Distribution')\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(vis_dir / 'split_feature_distributions.png') # Renamed from your list\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot time series of features\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    for i, feature in enumerate(FEATURE_COLUMNS, 1):\n",
    "        plt.subplot(2, 2, i)\n",
    "        plt.plot(train_data.index, train_data[feature], label='Train', alpha=0.7)\n",
    "        plt.plot(val_data.index, val_data[feature], label='Validation', alpha=0.7)\n",
    "        plt.plot(test_data.index, test_data[feature], label='Test', alpha=0.7)\n",
    "        plt.title(f'{feature} Time Series')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(vis_dir / 'split_feature_timeseries.png') # Renamed from your list\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot correlation matrix (using training data)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = train_data[FEATURE_COLUMNS].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix (Training Data)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(vis_dir / 'feature_correlation_matrix_train_data.png') # Specific to train data\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot anomaly distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    anomaly_counts = pd.DataFrame({\n",
    "        'Train': train_data[TARGET_COLUMN].value_counts(normalize=True) * 100, # Percentage\n",
    "        'Validation': val_data[TARGET_COLUMN].value_counts(normalize=True) * 100,\n",
    "        'Test': test_data[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "    }).fillna(0)\n",
    "    \n",
    "    anomaly_counts.plot(kind='bar')\n",
    "    plt.title('Anomaly Distribution Across Splits (% of total in split)')\n",
    "    plt.xlabel('Is Anomaly (0: Normal, 1: Anomaly)')\n",
    "    plt.ylabel('Percentage of samples in split (%)')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(vis_dir / 'anomaly_distribution_splits_percentage.png') # Renamed from your list\n",
    "    plt.close()\n",
    "\n",
    "# Execute data preparation when this cell is run\n",
    "prepared_data_dict = prepare_data()\n",
    "print(\"\\nData preparation script executed. Prepared data is available in `data`.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 1: Time Series Forecasting Models (LSTM & TCN)\n",
    "\n",
    "This section focuses on training and evaluating two types of neural networks for time series forecasting: Long Short-Term Memory (LSTM) and Temporal Convolutional Network (TCN). The goal is to predict future values of `numero_transazioni` (number of transactions) and `numero_transazioni_errate` (number of errored transactions) based on past observations. These models are trained exclusively on data identified as \"normal\" to learn baseline operational patterns.\n",
    "\n",
    "**What this cell does:**\n",
    "1.  **Setup:**\n",
    "    *   Sets Matplotlib and Seaborn plotting styles using `PLOT_PARAMS` from the configuration cell.\n",
    "    *   Configures TensorFlow global policies (disables JIT, sets mixed precision to 'float32' as per original script intent for this phase).\n",
    "2.  **Define Helper Functions:**\n",
    "    *   `print_training_info()`: Prints model summary and training start/elapsed time.\n",
    "    *   `create_lstm_model()`: Defines the architecture for the LSTM model. It includes two LSTM layers (the first bidirectional if using `tf.keras.layers.Bidirectional(LSTM(...))`, though the provided code uses `LSTM(...)` directly), Batch Normalization, Dropout layers, and Dense layers for output. It uses Adam optimizer and Huber loss.\n",
    "    *   `create_tcn_model()`: Defines the architecture for the TCN model. It uses a series of residual blocks with dilated causal convolutions, Batch Normalization, and Dropout. It also uses Adam optimizer and Huber loss.\n",
    "    *   `prepare_data_for_training()`: Selects the target columns (`PREDICTION_FEATURES`) from the y-arrays for training the forecasting models.\n",
    "    *   `train_forecasting_model()`:\n",
    "        *   Takes a model, training/validation data, model name, and model-specific parameters.\n",
    "        *   Implements callbacks: `EarlyStopping` (to prevent overfitting), `ModelCheckpoint` (to save the best model), `ReduceLROnPlateau` (to adjust learning rate), and `TensorBoard` (for logging).\n",
    "        *   Uses `tf.data.Dataset` for efficient data input pipelines.\n",
    "        *   Fits the model and returns the trained model, history, and training time.\n",
    "    *   `evaluate_forecasting_model()`:\n",
    "        *   Makes predictions on the test set.\n",
    "        *   Calculates MSE, MAE, and RMSE metrics on scaled predictions.\n",
    "        *   If a scaler is provided, it denormalizes the actual and predicted values for the `PREDICTION_FEATURES` and plots them.\n",
    "        *   Plots the distribution of prediction errors (denormalized).\n",
    "        *   Generates and plots future predictions for the next 30 minutes using a recursive approach.\n",
    "        *   Saves predictions, future forecasts, and metrics to files.\n",
    "3.  **`run_forecasting()` Function:**\n",
    "    *   Loads the preprocessed and sequenced forecasting data (`X_train_forecast`, `y_train_forecast`, etc.) and the `scaler_forecast`.\n",
    "    *   Prepares `y_train`, `y_val`, `y_test` to contain only the `PREDICTION_FEATURES`.\n",
    "    *   Instantiates, trains, and evaluates both the LSTM and TCN models using the helper functions.\n",
    "    *   Saves the trained LSTM and TCN models.\n",
    "    *   Plots a comparison of their training histories (loss vs. epoch).\n",
    "    *   Plots a bar chart comparing their evaluation metrics (MSE, MAE, RMSE) on the test set.\n",
    "4.  **Execution:** Calls `run_forecasting()` to perform all the above steps.\n",
    "\n",
    "**Why these choices were made:**\n",
    "*   **LSTM and TCN:** Chosen for their proven effectiveness in modeling sequential data and capturing temporal dependencies.\n",
    "*   **Training on Normal Data:** Forecasting models are trained on normal data to learn the system's baseline behavior. Significant deviations from these learned patterns during inference can indicate anomalies.\n",
    "*   **Huber Loss:** More robust to outliers in time series data compared to MSE.\n",
    "*   **Callbacks:** Essential for efficient and effective neural network training (preventing overfitting, saving best models, dynamic learning rate adjustment).\n",
    "*   **Detailed Evaluation:** Includes plotting actual vs. predicted values, error distributions, and future forecasts to provide a comprehensive understanding of model performance.\n",
    "*   **Model Comparison:** Directly comparing LSTM and TCN on the same task and data helps in selecting the better performing architecture for this specific problem.\n",
    "\n",
    "**Expected Output:**\n",
    "*   Print statements indicating the loading of data and scalers.\n",
    "*   For both LSTM and TCN models:\n",
    "    *   Model summaries (if `PRINT_PARAMS['show_summary']` is True).\n",
    "    *   Training progress (epochs, loss, validation loss, etc., if `PRINT_PARAMS['show_progress']` is True).\n",
    "    *   Training start/end times and elapsed times (if `PRINT_PARAMS['show_timing']` is True).\n",
    "    *   Evaluation metrics (MSE, MAE, RMSE) on the test set (if `PRINT_PARAMS['show_metrics']` is True).\n",
    "*   Saved plot files in the `results/` directory:\n",
    "    *   `lstm_forecast_vs_actual.png` & `tcn_forecast_vs_actual.png`: Actual vs. Predicted values for target features.\n",
    "    *   `lstm_error_distribution.png` & `tcn_error_distribution.png`: Distribution of prediction errors.\n",
    "    *   `lstm_future_forecast.png` & `tcn_future_forecast.png`: Forecasts for the next 30 minutes.\n",
    "    *   `forecasting_training_history.png`: Comparison of LSTM and TCN training/validation loss.\n",
    "    *   `forecasting_model_comparison.png`: Bar chart comparing LSTM and TCN evaluation metrics.\n",
    "*   Saved model files (`lstm_forecasting_model.keras`, `tcn_forecasting_model.keras`) in the `models/` directory.\n",
    "*   Saved prediction arrays and metric JSON files in the `results/` directory.\n",
    "*   A final message \"Forecasting models trained and evaluated.\"\n",
    "*   The cell will return a dictionary `forecasting_run_results` containing metrics and model objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase1 forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Time Series Forecasting Model Training and Evaluation\n",
    "\n",
    "plt.rcParams.update(PLOT_PARAMS['rc'])\n",
    "sns.set_context(PLOT_PARAMS['context'])\n",
    "sns.set_palette(PLOT_PARAMS['palette'])\n",
    "\n",
    "tf.config.optimizer.set_jit(False) \n",
    "tf.keras.mixed_precision.set_global_policy('float32') \n",
    "\n",
    "\n",
    "def print_training_info(model_name: str, model: Model, start_time: float):\n",
    "    \"\"\"Print training information.\"\"\"\n",
    "    if PRINT_PARAMS['show_summary']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        model.summary(line_length=120) \n",
    "    \n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTraining started at: {datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "def create_lstm_model(input_shape: Tuple[int, int]) -> Model:\n",
    "    \"\"\"Create LSTM model for time series forecasting.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(LSTM_PARAMS['units'], \n",
    "             input_shape=input_shape,\n",
    "             return_sequences=True,\n",
    "             dropout=LSTM_PARAMS['dropout'],\n",
    "             recurrent_dropout=0), \n",
    "        BatchNormalization(),\n",
    "        LSTM(LSTM_PARAMS['units'] // 2,\n",
    "             return_sequences=False,\n",
    "             dropout=LSTM_PARAMS['dropout'],\n",
    "             recurrent_dropout=0),\n",
    "        BatchNormalization(),\n",
    "        Dense(LSTM_PARAMS['units'] // 2, activation='relu'),\n",
    "        Dropout(LSTM_PARAMS['dropout']),\n",
    "        Dense(LSTM_PARAMS['units'] // 4, activation='relu'),\n",
    "        Dropout(LSTM_PARAMS['dropout']),\n",
    "        Dense(len(PREDICTION_FEATURES)) \n",
    "    ])\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=LSTM_PARAMS.get('learning_rate', 0.001), \n",
    "        beta_1=LSTM_PARAMS.get('beta_1', 0.9),\n",
    "        beta_2=LSTM_PARAMS.get('beta_2', 0.999),\n",
    "        epsilon=LSTM_PARAMS.get('epsilon', 1e-07)\n",
    "    )\n",
    "    model.compile(optimizer=optimizer, loss='huber')\n",
    "    return model\n",
    "\n",
    "def create_tcn_model(input_shape: Tuple[int, int]) -> Model:\n",
    "    \"\"\"Create Temporal Convolutional Network model.\"\"\"\n",
    "    def residual_block(x, dilation_rate, nb_filters, kernel_size, dropout_rate=0.1):\n",
    "        y = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(x)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Activation('relu')(y)\n",
    "        y = Dropout(dropout_rate)(y)\n",
    "        \n",
    "        y = Conv1D(filters=nb_filters, kernel_size=kernel_size, dilation_rate=dilation_rate, padding='causal')(y)\n",
    "        y = BatchNormalization()(y)\n",
    "        y = Dropout(dropout_rate)(y)\n",
    "        \n",
    "        if x.shape[-1] != nb_filters:\n",
    "            x_reshaped = Conv1D(nb_filters, 1, padding='same')(x) \n",
    "        else:\n",
    "            x_reshaped = x\n",
    "        \n",
    "        out = Add()([x_reshaped, y])\n",
    "        out = Activation('relu')(out)\n",
    "        return out\n",
    "    \n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    for _ in range(TCN_PARAMS.get('nb_stacks', 1)): \n",
    "        for dilation_rate in TCN_PARAMS['dilations']:\n",
    "            x = residual_block(\n",
    "                x,\n",
    "                dilation_rate=dilation_rate,\n",
    "                nb_filters=TCN_PARAMS['nb_filters'],\n",
    "                kernel_size=TCN_PARAMS['kernel_size'],\n",
    "                dropout_rate=0.1 \n",
    "            )\n",
    "    \n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    \n",
    "    x = Dense(TCN_PARAMS['nb_filters'] // 2, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(TCN_PARAMS['nb_filters'] // 4, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    outputs = Dense(len(PREDICTION_FEATURES))(x) \n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=TCN_PARAMS.get('learning_rate', 0.001),\n",
    "        beta_1=TCN_PARAMS.get('beta_1', 0.9),\n",
    "        beta_2=TCN_PARAMS.get('beta_2', 0.999),\n",
    "        epsilon=TCN_PARAMS.get('epsilon', 1e-07)\n",
    "    )\n",
    "    model.compile(optimizer=optimizer, loss='huber')\n",
    "    return model\n",
    "\n",
    "def prepare_data_for_training(X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Prepare data for training, using all features for input but only prediction features for output.\"\"\"\n",
    "    X_train = X\n",
    "    feature_indices = [FEATURE_COLUMNS.index(f) for f in PREDICTION_FEATURES]\n",
    "    y_train = y[:, feature_indices]\n",
    "    return X_train, y_train\n",
    "\n",
    "def train_forecasting_model(model: Model, \n",
    "                          X_train: np.ndarray, \n",
    "                          y_train: np.ndarray,\n",
    "                          X_val: np.ndarray,\n",
    "                          y_val: np.ndarray,\n",
    "                          model_name: str,\n",
    "                          model_params: dict) -> Dict[str, Any]:\n",
    "    \"\"\"Train the forecasting model.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print_training_info(model_name, model, start_time)\n",
    "    \n",
    "    def lr_schedule(epoch, lr): \n",
    "        initial_lr = model_params.get('learning_rate', 0.001)\n",
    "        if epoch < 10:\n",
    "            return float(initial_lr)\n",
    "        else:\n",
    "            return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=model_params.get('patience',15), \n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(MODELS_DIR / f'{model_name}_best.keras'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5, \n",
    "            min_lr=1e-6, \n",
    "            mode='min',\n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard(\n",
    "            log_dir=str(RESULTS_DIR / 'logs' / model_name.lower()), \n",
    "            histogram_freq=TENSORBOARD_PARAMS.get('histogram_freq', 1),\n",
    "            update_freq=TENSORBOARD_PARAMS.get('update_freq', 'epoch'),\n",
    "            profile_batch=TENSORBOARD_PARAMS.get('profile_batch', 0) \n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    batch_size = model_params.get('batch_size', 64)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\\\n",
    "        .shuffle(buffer_size=len(X_train))\\\n",
    "        .batch(batch_size)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\\\n",
    "        .batch(batch_size)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=model_params.get('epochs', 50),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1 if PRINT_PARAMS['show_progress'] else 0\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        print(f\"\\nTraining for {model_name} completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history.history,\n",
    "        'training_time': total_time\n",
    "    }\n",
    "\n",
    "def evaluate_forecasting_model(model: Model,\n",
    "                             X_test: np.ndarray,\n",
    "                             y_test: np.ndarray, \n",
    "                             model_name: str,\n",
    "                             scaler=None,\n",
    "                             timestamps_for_sequences=None) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate the forecasting model.\"\"\"\n",
    "    eval_start_time = time.time()\n",
    "    \n",
    "    y_pred_scaled = model.predict(X_test, batch_size=LSTM_PARAMS.get('batch_size',64)) \n",
    "    \n",
    "    mse = np.mean((y_test - y_pred_scaled) ** 2)\n",
    "    mae = np.mean(np.abs(y_test - y_pred_scaled))\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    if scaler is not None:\n",
    "        y_test_full_shape = np.zeros((len(y_test), len(FEATURE_COLUMNS)))\n",
    "        y_pred_full_shape = np.zeros((len(y_pred_scaled), len(FEATURE_COLUMNS)))\n",
    "        \n",
    "        feature_indices = [FEATURE_COLUMNS.index(f) for f in PREDICTION_FEATURES]\n",
    "        \n",
    "        y_test_full_shape[:, feature_indices] = y_test\n",
    "        y_pred_full_shape[:, feature_indices] = y_pred_scaled\n",
    "        \n",
    "        y_test_denorm_full = scaler.inverse_transform(y_test_full_shape)\n",
    "        y_pred_denorm_full = scaler.inverse_transform(y_pred_full_shape)\n",
    "        \n",
    "        y_test_denorm = y_test_denorm_full[:, feature_indices]\n",
    "        y_pred_denorm = y_pred_denorm_full[:, feature_indices]\n",
    "    else:\n",
    "        y_test_denorm = y_test\n",
    "        y_pred_denorm = y_pred_scaled\n",
    "    \n",
    "    plt.figure(figsize=(15, 4 * len(PREDICTION_FEATURES)))\n",
    "    for i, feature_name_to_plot in enumerate(PREDICTION_FEATURES):\n",
    "        plt.subplot(len(PREDICTION_FEATURES), 1, i + 1)\n",
    "        plt.plot(y_test_denorm[:, i], label='Actual', alpha=0.7)\n",
    "        plt.plot(y_pred_denorm[:, i], label='Predicted', alpha=0.7)\n",
    "        plt.title(f'{feature_name_to_plot} - {model_name} Forecast')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name.lower()}_forecast_vs_actual.png') \n",
    "    plt.close()\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    errors_denorm = y_test_denorm - y_pred_denorm \n",
    "    for i, feature_name_to_plot in enumerate(PREDICTION_FEATURES):\n",
    "        plt.subplot(1, len(PREDICTION_FEATURES), i + 1) \n",
    "        sns.histplot(errors_denorm[:, i], kde=True)\n",
    "        plt.title(f'{feature_name_to_plot} Error Distribution (Denormalized)')\n",
    "        plt.xlabel('Error')\n",
    "        plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name.lower()}_error_distribution.png') \n",
    "    plt.close()\n",
    "    \n",
    "    last_sequence_scaled = X_test[-1:] \n",
    "    future_steps = FORECAST_HORIZON \n",
    "    future_predictions_scaled = []\n",
    "    current_sequence_scaled = last_sequence_scaled.copy()\n",
    "\n",
    "    for _ in range(future_steps):\n",
    "        next_pred_scaled_for_pred_features = model.predict(current_sequence_scaled, verbose=0)[0] \n",
    "        future_predictions_scaled.append(next_pred_scaled_for_pred_features)\n",
    "        \n",
    "        new_time_step_scaled = current_sequence_scaled[0, -1, :].copy() \n",
    "        feature_indices_for_update = [FEATURE_COLUMNS.index(f) for f in PREDICTION_FEATURES]\n",
    "        new_time_step_scaled[feature_indices_for_update] = next_pred_scaled_for_pred_features\n",
    "\n",
    "        current_sequence_scaled = np.roll(current_sequence_scaled, -1, axis=1)\n",
    "        current_sequence_scaled[0, -1, :] = new_time_step_scaled\n",
    "    \n",
    "    future_predictions_scaled = np.array(future_predictions_scaled)\n",
    "    \n",
    "    if scaler is not None:\n",
    "        future_predictions_full_shape = np.zeros((len(future_predictions_scaled), len(FEATURE_COLUMNS)))\n",
    "        # This line for feature_indices should be outside the loop if it's constant\n",
    "        feature_indices = [FEATURE_COLUMNS.index(f) for f in PREDICTION_FEATURES] \n",
    "        future_predictions_full_shape[:, feature_indices] = future_predictions_scaled\n",
    "        future_predictions_denorm_full = scaler.inverse_transform(future_predictions_full_shape)\n",
    "        future_predictions_denorm = future_predictions_denorm_full[:, feature_indices]\n",
    "    else:\n",
    "        future_predictions_denorm = future_predictions_scaled\n",
    "    \n",
    "    future_plot_timestamps = None\n",
    "    if timestamps_for_sequences is not None and len(timestamps_for_sequences) > 0:\n",
    "        \n",
    "        last_actual_timestamp = pd.to_datetime(timestamps_for_sequences[len(y_test)-1]) \n",
    "        future_plot_timestamps = pd.date_range(start=last_actual_timestamp + pd.Timedelta(minutes=1), periods=future_steps, freq='T')\n",
    "\n",
    "    plt.figure(figsize=(15, 4 * len(PREDICTION_FEATURES)))\n",
    "    for i, feature_name_to_plot in enumerate(PREDICTION_FEATURES):\n",
    "        plt.subplot(len(PREDICTION_FEATURES), 1, i + 1)\n",
    "        if future_plot_timestamps is not None:\n",
    "            plt.plot(future_plot_timestamps, future_predictions_denorm[:, i], \n",
    "                     label='Future Predictions', alpha=0.7, linestyle='--', marker='o')\n",
    "            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%H:%M'))\n",
    "            plt.gca().xaxis.set_major_locator(mdates.MinuteLocator(interval=5 if future_steps > 10 else 1))\n",
    "            plt.xticks(rotation=30, ha='right')\n",
    "        else:\n",
    "            plt.plot(future_predictions_denorm[:, i], \n",
    "                     label='Future Predictions', alpha=0.7, linestyle='--', marker='o')\n",
    "        \n",
    "        plt.title(f'{feature_name_to_plot} - {model_name} Future Forecast (Next {future_steps} Minutes)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(f\"Predicted {feature_name_to_plot}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name.lower()}_future_forecast.png') \n",
    "    plt.close()\n",
    "    \n",
    "    if PRINT_PARAMS['show_metrics']:\n",
    "        print(f\"\\n{model_name} Evaluation Metrics (on scaled test data):\")\n",
    "        print(f\"MSE: {mse:.{PRINT_PARAMS['precision']}f}\")\n",
    "        print(f\"MAE: {mae:.{PRINT_PARAMS['precision']}f}\")\n",
    "        print(f\"RMSE: {rmse:.{PRINT_PARAMS['precision']}f}\")\n",
    "    \n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        eval_time = time.time() - eval_start_time\n",
    "        print(f\"Evaluation for {model_name} completed in {eval_time:.2f} seconds\")\n",
    "    \n",
    "    np.save(RESULTS_DIR / f'{model_name.lower()}_test_predictions_denorm.npy', y_pred_denorm)\n",
    "    np.save(RESULTS_DIR / f'{model_name.lower()}_future_predictions_denorm.npy', future_predictions_denorm)\n",
    "    \n",
    "    metrics_to_save = {\n",
    "        'mse_scaled': float(mse),\n",
    "        'mae_scaled': float(mae),\n",
    "        'rmse_scaled': float(rmse)\n",
    "    }\n",
    "    if scaler is not None:\n",
    "        metrics_to_save['mse_denorm'] = float(np.mean((y_test_denorm - y_pred_denorm) ** 2))\n",
    "        metrics_to_save['mae_denorm'] = float(np.mean(np.abs(y_test_denorm - y_pred_denorm)))\n",
    "        metrics_to_save['rmse_denorm'] = float(np.sqrt(metrics_to_save['mse_denorm']))\n",
    "\n",
    "    with open(RESULTS_DIR / f'{model_name.lower()}_metrics.json', 'w') as f:\n",
    "        json.dump(metrics_to_save, f, indent=4)\n",
    "    \n",
    "    return metrics_to_save\n",
    "\n",
    "def run_forecasting():\n",
    "    \"\"\"Main function to run the forecasting phase.\"\"\"\n",
    "    print(\"\\nLoading prepared data for forecasting...\")\n",
    "    start_time_main = time.time()\n",
    "    \n",
    "    X_train = np.load(DATA_DIR / 'X_train_forecast.npy')\n",
    "    y_train_full = np.load(DATA_DIR / 'y_train_forecast.npy') \n",
    "    X_val = np.load(DATA_DIR / 'X_val_forecast.npy')\n",
    "    y_val_full = np.load(DATA_DIR / 'y_val_forecast.npy')\n",
    "    X_test = np.load(DATA_DIR / 'X_test_forecast.npy')\n",
    "    y_test_full = np.load(DATA_DIR / 'y_test_forecast.npy')\n",
    "    \n",
    "    _, y_train = prepare_data_for_training(X_train, y_train_full)\n",
    "    _, y_val = prepare_data_for_training(X_val, y_val_full)\n",
    "    _, y_test = prepare_data_for_training(X_test, y_test_full)\n",
    "\n",
    "    timestamps_for_test_sequences = None\n",
    "    if (DATA_DIR / 'test_timestamps_for_sequences.npy').exists():\n",
    "        timestamps_for_test_sequences = pd.to_datetime(np.load(DATA_DIR / 'test_timestamps_for_sequences.npy'))\n",
    "        # Ensure timestamps align with the y_test data (which is shorter than original test_data due to sequencing)\n",
    "        if len(timestamps_for_test_sequences) > len(y_test):\n",
    "             timestamps_for_test_sequences = timestamps_for_test_sequences[:len(y_test)]\n",
    "        print(f\"Test sequence timestamps loaded successfully. Length: {len(timestamps_for_test_sequences)}\")\n",
    "    else:\n",
    "        print(\"Warning: test_timestamps_for_sequences.npy not found.\")\n",
    "            \n",
    "    scaler = None\n",
    "    if (MODELS_DIR / 'scaler_forecast.joblib').exists():\n",
    "        scaler = joblib.load(MODELS_DIR / 'scaler_forecast.joblib')\n",
    "        print(\"Forecasting scaler loaded successfully.\")\n",
    "    else:\n",
    "        print(f\"Warning: scaler_forecast.joblib not found in {MODELS_DIR}. Evaluation will use scaled values for plots.\")\n",
    "\n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        print(f\"Data loaded in {time.time() - start_time_main:.2f} seconds\")\n",
    "    \n",
    "    all_metrics = {}\n",
    "\n",
    "    input_shape_common = (X_train.shape[1], X_train.shape[2])\n",
    "    \n",
    "    lstm_model = create_lstm_model(input_shape=input_shape_common)\n",
    "    lstm_results = train_forecasting_model(lstm_model, X_train, y_train, X_val, y_val, 'LSTM', LSTM_PARAMS)\n",
    "    all_metrics['lstm'] = evaluate_forecasting_model(lstm_results['model'], X_test, y_test, 'LSTM', scaler, timestamps_for_test_sequences)\n",
    "    save_model(lstm_results['model'], 'lstm_forecasting_model') \n",
    "\n",
    "    tcn_model = create_tcn_model(input_shape=input_shape_common) \n",
    "    tcn_results = train_forecasting_model(tcn_model, X_train, y_train, X_val, y_val, 'TCN', TCN_PARAMS)\n",
    "    all_metrics['tcn'] = evaluate_forecasting_model(tcn_results['model'], X_test, y_test, 'TCN', scaler, timestamps_for_test_sequences)\n",
    "    save_model(tcn_results['model'], 'tcn_forecasting_model') \n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(lstm_results['history']['loss'], label='LSTM Train Loss')\n",
    "    plt.plot(lstm_results['history']['val_loss'], label='LSTM Val Loss')\n",
    "    plt.title('LSTM Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (Huber)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(tcn_results['history']['loss'], label='TCN Train Loss')\n",
    "    plt.plot(tcn_results['history']['val_loss'], label='TCN Val Loss')\n",
    "    plt.title('TCN Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (Huber)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'forecasting_training_history.png') \n",
    "    plt.close()\n",
    "    \n",
    "    metric_to_compare = 'mae_denorm' if 'mae_denorm' in all_metrics['lstm'] else 'mae_scaled'\n",
    "    \n",
    "    # Ensure keys exist before trying to access them for plotting\n",
    "    metrics_for_plot = []\n",
    "    if 'mse_denorm' in all_metrics['lstm'] and 'mae_denorm' in all_metrics['lstm'] and 'rmse_denorm' in all_metrics['lstm']:\n",
    "        metrics_for_plot = ['mse_denorm', 'mae_denorm', 'rmse_denorm']\n",
    "    elif 'mse_scaled' in all_metrics['lstm'] and 'mae_scaled' in all_metrics['lstm'] and 'rmse_scaled' in all_metrics['lstm']:\n",
    "        metrics_for_plot = ['mse_scaled', 'mae_scaled', 'rmse_scaled']\n",
    "    else:\n",
    "        print(\"Warning: Not enough metrics found to generate comparison plot.\")\n",
    "\n",
    "\n",
    "    if metrics_for_plot: # Only plot if we have metrics\n",
    "        lstm_plot_values = [all_metrics['lstm'].get(m, np.nan) for m in metrics_for_plot]\n",
    "        tcn_plot_values = [all_metrics['tcn'].get(m, np.nan) for m in metrics_for_plot]\n",
    "\n",
    "        x_labels = [m.replace('_denorm','').replace('_scaled','').upper() for m in metrics_for_plot]\n",
    "        x_indices = np.arange(len(x_labels))\n",
    "        bar_width = 0.35\n",
    "        \n",
    "        \n",
    "        palette_name_from_config = PLOT_PARAMS.get('palette', 'deep')\n",
    "        actual_color_palette = sns.color_palette(palette_name_from_config)\n",
    "\n",
    "        fig_comp, ax_comp = plt.subplots(figsize=(12, 6))\n",
    "        ax_comp.bar(x_indices - bar_width/2, lstm_plot_values, bar_width, label='LSTM', color=actual_color_palette[0])\n",
    "        ax_comp.bar(x_indices + bar_width/2, tcn_plot_values, bar_width, label='TCN', color=actual_color_palette[1])\n",
    "        \n",
    "        ax_comp.set_xlabel('Metrics')\n",
    "        ax_comp.set_ylabel('Value')\n",
    "        ax_comp.set_title(f'Forecasting Model Comparison ({ \"Denormalized\" if \"denorm\" in metrics_for_plot[0] else \"Scaled\" })')\n",
    "        ax_comp.set_xticks(x_indices)\n",
    "        ax_comp.set_xticklabels(x_labels)\n",
    "        ax_comp.legend()\n",
    "        ax_comp.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / 'forecasting_model_comparison.png') \n",
    "        plt.close(fig_comp)\n",
    "\n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        print(f\"\\nTotal forecasting phase execution time: {time.time() - start_time_main:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'lstm_metrics': all_metrics.get('lstm', {}), \n",
    "        'tcn_metrics': all_metrics.get('tcn', {}),\n",
    "        'lstm_model': lstm_results.get('model') if 'lstm_results' in locals() else None, \n",
    "        'tcn_model': tcn_results.get('model') if 'tcn_results' in locals() else None,   \n",
    "        'total_time': time.time() - start_time_main\n",
    "    }\n",
    "\n",
    "forecasting_run_results = run_forecasting()\n",
    "print(\"\\nForecasting models trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 2: Anomaly Detection with LSTM Autoencoder\n",
    "\n",
    "This section focuses on training and evaluating an LSTM-based Autoencoder specifically for anomaly detection. The Autoencoder is trained only on \"normal\" data sequences. The underlying principle is that the model will learn to reconstruct normal patterns with low error, while anomalous patterns will result in higher reconstruction errors, allowing for their identification.\n",
    "\n",
    "**What this cell does:**\n",
    "1.  **Setup & Utility Functions (Defined within this cell for self-containment regarding plotting):**\n",
    "    *   Sets Matplotlib/Seaborn plotting styles and TensorFlow mixed precision policy.\n",
    "    *   **`plot_confusion_matrix()`**, **`plot_roc_curve()`**, **`plot_precision_recall_curve()`**, **`plot_anomaly_scores_distribution()`**: These are local copies of the plotting utilities specifically for visualizing the Autoencoder's anomaly detection performance. They generate and save relevant plots to the `RESULTS_DIR`.\n",
    "2.  **Define Autoencoder-specific Helper Functions:**\n",
    "    *   `print_training_info_ad()`: Similar to the forecasting version, but for the Autoencoder training.\n",
    "    *   `create_autoencoder()`: Defines the architecture of the LSTM-based Autoencoder. It consists of an encoder path with Bidirectional LSTM layers compressing the input sequence into a bottleneck representation, and a decoder path with Bidirectional LSTM layers attempting to reconstruct the original sequence from this representation. It uses Adam optimizer and MSE loss.\n",
    "    *   `plot_training_history_ad()`: Plots the training and validation loss (MSE) and MAE for the Autoencoder over epochs.\n",
    "    *   `train_autoencoder()`:\n",
    "        *   Manages the training process for the Autoencoder, using callbacks like `EarlyStopping`, `ModelCheckpoint`, and `ReduceLROnPlateau`.\n",
    "        *   Trains the model using normal data sequences from the training set (`X_train_ad`) as both input and target.\n",
    "        *   Uses a validation set (`X_val_ad_mixed`, also using its inputs as targets for reconstruction loss) to monitor for overfitting.\n",
    "    *   `detect_anomalies_autoencoder()`:\n",
    "        *   Calculates reconstruction errors (Mean Squared Error) for the validation set (`X_val_for_thresh`).\n",
    "        *   Performs a threshold analysis: iterates through various percentile-based thresholds derived from validation errors and calculates Precision, Recall, and F1-score against the true validation labels (`y_val_for_thresh_labels`).\n",
    "        *   Saves this threshold analysis to `autoencoder_threshold_analysis.csv`.\n",
    "        *   Selects the optimal threshold based on maximizing the specified `optimization_metric` (e.g., F1-score) on the validation set.\n",
    "        *   Calculates reconstruction errors for the test set (`X_test`).\n",
    "        *   Classifies test instances as anomalous if their reconstruction error exceeds the chosen optimal threshold.\n",
    "    *   `evaluate_anomaly_detection_ad()`:\n",
    "        *   Takes true labels, predicted binary anomaly labels, and raw anomaly scores for the test set.\n",
    "        *   Calculates and prints a detailed classification report (Precision, Recall, F1-score for normal and anomaly classes).\n",
    "        *   Calls the local plotting functions (`plot_confusion_matrix`, `plot_roc_curve`, `plot_precision_recall_curve`, `plot_anomaly_scores_distribution`) to visualize test set performance.\n",
    "        *   Saves the performance metrics to `all_anomaly_metrics.csv` (appending/overwriting for the \"Autoencoder\" model) and a model-specific JSON file.\n",
    "3.  **`run_anomaly_detection()` Function:**\n",
    "    *   Loads the preprocessed data specifically prepared for the Autoencoder (`X_train_ad`, `X_val_ad_mixed`, `y_val_ad_labels`, `X_test_ad_mixed`, `y_test_ad_labels`).\n",
    "    *   Instantiates, trains (using `train_autoencoder`), and then uses the trained Autoencoder to detect anomalies on the test set (using `detect_anomalies_autoencoder`).\n",
    "    *   Evaluates the anomaly detection performance on the test set (using `evaluate_anomaly_detection_ad`).\n",
    "    *   Saves the trained Autoencoder model.\n",
    "4.  **Execution:** Calls `run_anomaly_detection()` to perform all the Autoencoder-related steps.\n",
    "\n",
    "**Why these choices were made:**\n",
    "*   **LSTM Autoencoder:** A common and effective deep learning approach for unsupervised/semi-supervised anomaly detection in sequential data. It learns a compressed representation of normal data.\n",
    "*   **Training on Normal Data Only:** This is a core principle for this type of anomaly detection. The model should only learn what \"normal\" looks like.\n",
    "*   **Reconstruction Error as Anomaly Score:** Deviations from learned normal patterns result in higher reconstruction errors, which serve as the anomaly score.\n",
    "*   **Threshold Optimization on Validation Set:** Crucial for converting continuous anomaly scores into binary anomaly/normal classifications. Using the validation set prevents data leakage from the test set into the threshold selection process and aims for a threshold that generalizes well. Maximizing F1-score is a common strategy to balance precision and recall.\n",
    "*   **Comprehensive Evaluation:** Using Precision, Recall, F1-score, AUC-ROC, AUC-PR, and confusion matrices provides a thorough assessment of the anomaly detection performance.\n",
    "\n",
    "**Expected Output:**\n",
    "*   Print statements indicating data loading and model training stages.\n",
    "*   Autoencoder model summary.\n",
    "*   Training progress (epochs, loss, validation loss, MAE, validation MAE).\n",
    "*   Results of the threshold analysis on the validation set printed as a table.\n",
    "*   The chosen optimal threshold and its corresponding F1-score (or other optimization metric).\n",
    "*   Classification report for the test set performance.\n",
    "*   Saved plot files in the `results/` directory:\n",
    "    *   `Autoencoder_Training_History.png`\n",
    "    *   `Autoencoder_Anomaly_Scores_Distribution.png`\n",
    "    *   `Autoencoder_Confusion_Matrix.png`\n",
    "    *   `Autoencoder_ROC_Curve.png`\n",
    "    *   `Autoencoder_Precision_Recall_Curve.png`\n",
    "    *   (Possibly `Autoencoder_Test_Set_Threshold_Analysis.png` if `plot_anomaly_results` from utils was called with that specific name)\n",
    "*   Saved `autoencoder_threshold_analysis.csv`.\n",
    "*   Saved `all_anomaly_metrics.csv` (updated or created).\n",
    "*   Saved `autoencoder_metrics_ad.json`.\n",
    "*   Saved the Autoencoder model (`autoencoder_anomaly_model.keras`) in the `models/` directory.\n",
    "*   A final message \"Autoencoder Anomaly Detection phase completed!\"\n",
    "*   The cell will return a dictionary `autoencoder_evaluation_metrics` containing key performance indicators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase2 Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Anomaly Detection Model (Autoencoder) Training and Evaluation\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score, precision_score, recall_score, f1_score, classification_report, roc_auc_score\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path \n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true: np.ndarray, y_pred: np.ndarray, model_name: str):\n",
    "    \"\"\"Plot confusion matrix for anomaly detection results.\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'{model_name} - Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name}_Confusion_Matrix.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve(y_true: np.ndarray, scores: np.ndarray, model_name: str):\n",
    "    \"\"\"Plot ROC curve for anomaly detection results.\"\"\"\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'{model_name} - ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name}_ROC_Curve.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_precision_recall_curve(y_true: np.ndarray, scores: np.ndarray, model_name: str):\n",
    "    \"\"\"Plot precision-recall curve for anomaly detection results.\"\"\"\n",
    "   \n",
    "    precision_curve_vals, recall_curve_vals, _ = precision_recall_curve(y_true, scores) # Renamed to avoid conflict\n",
    "    avg_precision = average_precision_score(y_true, scores)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall_curve_vals, precision_curve_vals, label=f'PR curve (AP = {avg_precision:.3f})')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'{model_name} - Precision-Recall Curve')\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name}_Precision_Recall_Curve.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_anomaly_scores_distribution(y_true: np.ndarray, scores: np.ndarray, model_name: str):\n",
    "    \"\"\"Plot distribution of anomaly scores by class with enhanced visualization.\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    df_plot = pd.DataFrame({ \n",
    "        'Score': scores,\n",
    "        'Class': ['Anomaly' if y == 1 else 'Normal' for y in y_true]\n",
    "    })\n",
    "    \n",
    "    gs = plt.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    sns.histplot(data=df_plot, x='Score', hue='Class', kde=True, \n",
    "                palette={'Normal': 'blue', 'Anomaly': 'red'},\n",
    "                alpha=0.6, bins=50, ax=ax1) \n",
    "    \n",
    "    for cls_name in ['Normal', 'Anomaly']:\n",
    "        mean_score = df_plot[df_plot['Class'] == cls_name]['Score'].mean()\n",
    "        if not pd.isna(mean_score): \n",
    "            ax1.axvline(x=mean_score, color='red' if cls_name == 'Anomaly' else 'blue',\n",
    "                       linestyle='--', alpha=0.8,\n",
    "                       label=f'{cls_name} Mean: {mean_score:.4f}')\n",
    "    \n",
    "    ax1.set_title(f'{model_name} - Anomaly Scores Distribution', pad=20)\n",
    "    ax1.set_xlabel('Anomaly Score')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = plt.subplot(gs[1])\n",
    "    sns.boxplot(data=df_plot, x='Class', y='Score', palette={'Normal': 'blue', 'Anomaly': 'red'}, ax=ax2) # Pass ax\n",
    "    ax2.set_title('Score Distribution by Class')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    stats_text = f\"\"\"\n",
    "    Statistics:\n",
    "    Normal samples: {len(df_plot[df_plot['Class'] == 'Normal'])}\n",
    "    Anomaly samples: {len(df_plot[df_plot['Class'] == 'Anomaly'])}\n",
    "    Normal mean: {df_plot[df_plot['Class'] == 'Normal']['Score'].mean():.4f}\n",
    "    Anomaly mean: {df_plot[df_plot['Class'] == 'Anomaly']['Score'].mean():.4f}\n",
    "    Normal std: {df_plot[df_plot['Class'] == 'Normal']['Score'].std():.4f}\n",
    "    Anomaly std: {df_plot[df_plot['Class'] == 'Anomaly']['Score'].std():.4f}\n",
    "    \"\"\"\n",
    "    plt.figtext(0.02, 0.02, stats_text, fontsize=10, bbox=dict(facecolor='white', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name}_Anomaly_Scores_Distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "plt.rcParams.update(PLOT_PARAMS['rc'])\n",
    "sns.set_context(PLOT_PARAMS['context'])\n",
    "\n",
    "\n",
    "tf.keras.mixed_precision.set_global_policy('mixed_float16') \n",
    "\n",
    "def print_training_info_ad(model_name: str, model: Model, start_time: float): \n",
    "    \"\"\"Print training information.\"\"\"\n",
    "    if PRINT_PARAMS['show_summary']:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training {model_name}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        if model is not None:\n",
    "            model.summary(line_length=120)\n",
    "    \n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTraining started at: {datetime.fromtimestamp(start_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"Elapsed time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "def create_autoencoder(input_shape: Tuple[int, int]) -> Model:\n",
    "    \"\"\"Create an improved autoencoder model for anomaly detection.\"\"\"\n",
    "    encoder_input = Input(shape=input_shape)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=AUTOENCODER_PARAMS.get('dropout', 0.2)))(encoder_input)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(64, return_sequences=False, dropout=AUTOENCODER_PARAMS.get('dropout', 0.2)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    encoded = Dense(AUTOENCODER_PARAMS.get('units', [64,32,16])[-1], activation='relu')(x) \n",
    "    encoded = BatchNormalization()(encoded)\n",
    "    \n",
    "    x = Dense(64, activation='relu')(encoded) \n",
    "    x = BatchNormalization()(x)\n",
    "    x = RepeatVector(input_shape[0])(x) \n",
    "    x = Bidirectional(LSTM(64, return_sequences=True, dropout=AUTOENCODER_PARAMS.get('dropout', 0.2)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Bidirectional(LSTM(128, return_sequences=True, dropout=AUTOENCODER_PARAMS.get('dropout', 0.2)))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    decoded = TimeDistributed(Dense(input_shape[1], activation='linear'))(x) \n",
    "    \n",
    "    autoencoder = Model(encoder_input, decoded)\n",
    "    optimizer = tf.keras.optimizers.Adam(\n",
    "        learning_rate=AUTOENCODER_PARAMS.get('learning_rate', 0.001),\n",
    "        beta_1=AUTOENCODER_PARAMS.get('beta_1', 0.9),\n",
    "        beta_2=AUTOENCODER_PARAMS.get('beta_2', 0.999),\n",
    "        epsilon=AUTOENCODER_PARAMS.get('epsilon', 1e-07)\n",
    "    )\n",
    "    autoencoder.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return autoencoder\n",
    "\n",
    "def plot_training_history_ad(history: Dict[str, Any], model_name: str): \n",
    "    \"\"\"Plot training history of the autoencoder model.\"\"\"\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Loss (MSE)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['mae'], label='Training MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.title(f'{model_name} - MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'{model_name}_Training_History.png') \n",
    "    plt.close()\n",
    "\n",
    "def train_autoencoder(model: Model, X_train: np.ndarray, X_val: np.ndarray) -> Dict[str, Any]:\n",
    "    \"\"\"Train the autoencoder model with improved settings.\"\"\"\n",
    "    start_time = time.time()\n",
    "    print_training_info_ad(\"Autoencoder\", model, start_time)\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=AUTOENCODER_PARAMS.get('patience', 10), \n",
    "            restore_best_weights=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            filepath=str(MODELS_DIR / 'autoencoder_best.keras'),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True,\n",
    "            mode='min'\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5, \n",
    "            patience=5,  \n",
    "            min_lr=1e-6, \n",
    "            verbose=1\n",
    "        ),\n",
    "        TensorBoard( \n",
    "            log_dir=str(RESULTS_DIR / 'logs' / 'autoencoder'),\n",
    "            histogram_freq=TENSORBOARD_PARAMS.get('histogram_freq',1),\n",
    "            update_freq=TENSORBOARD_PARAMS.get('update_freq','epoch'),\n",
    "            profile_batch=TENSORBOARD_PARAMS.get('profile_batch',0)\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    batch_size = AUTOENCODER_PARAMS.get('batch_size', 64)\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, X_train))\\\n",
    "        .shuffle(buffer_size=len(X_train))\\\n",
    "        .batch(batch_size)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, X_val))\\\n",
    "        .batch(batch_size)\\\n",
    "        .prefetch(tf.data.AUTOTUNE)\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=AUTOENCODER_PARAMS.get('epochs', 50),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1 if PRINT_PARAMS['show_progress'] else 0\n",
    "    )\n",
    "    \n",
    "    plot_training_history_ad(history.history, \"Autoencoder\") \n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        print(f\"\\nAutoencoder training completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history.history,\n",
    "        'training_time': total_time\n",
    "    }\n",
    "\n",
    "def detect_anomalies_autoencoder(model: Model, \n",
    "                               X_test: np.ndarray, \n",
    "                               X_val_for_thresh: np.ndarray, \n",
    "                               y_val_for_thresh_labels: np.ndarray, \n",
    "                               threshold_percentiles_list: List[float], \n",
    "                               optimization_metric: str = 'f1' \n",
    "                               ) -> Tuple[np.ndarray, np.ndarray, float]: \n",
    "    \"\"\"Detect anomalies using the autoencoder model, with threshold optimized on validation data.\"\"\"\n",
    "    \n",
    "    print(\"\\nCalculating reconstruction errors on validation set for threshold optimization...\")\n",
    "    val_predictions = model.predict(X_val_for_thresh, batch_size=AUTOENCODER_PARAMS.get('batch_size', 256), verbose=0)\n",
    "    val_reconstruction_errors = np.mean(np.square(X_val_for_thresh - val_predictions), axis=(1, 2))\n",
    "    \n",
    "    y_val_binary_labels = (np.any(y_val_for_thresh_labels > 0, axis=1)).astype(int) if len(y_val_for_thresh_labels.shape) > 1 and y_val_for_thresh_labels.shape[1] > 1 else (y_val_for_thresh_labels > 0.5).astype(int)\n",
    "    \n",
    "    if len(y_val_binary_labels) != len(val_reconstruction_errors):\n",
    "         raise ValueError(f\"Length mismatch: y_val_binary_labels ({len(y_val_binary_labels)}) vs val_reconstruction_errors ({len(val_reconstruction_errors)})\")\n",
    "\n",
    "    print(\"\\nAnalyzing different thresholds for Autoencoder on validation data:\")\n",
    "    threshold_analysis_results = []\n",
    "    candidate_thresholds = np.percentile(val_reconstruction_errors, threshold_percentiles_list)\n",
    "    \n",
    "    for thresh_val in candidate_thresholds:\n",
    "        y_pred_val_binary = (val_reconstruction_errors > thresh_val).astype(int)\n",
    "        p = precision_score(y_val_binary_labels, y_pred_val_binary, zero_division=0)\n",
    "        r = recall_score(y_val_binary_labels, y_pred_val_binary, zero_division=0)\n",
    "        f1 = f1_score(y_val_binary_labels, y_pred_val_binary, zero_division=0)\n",
    "        cm_thresh = confusion_matrix(y_val_binary_labels, y_pred_val_binary)\n",
    "        fp_thresh = cm_thresh[0,1] if cm_thresh.shape == (2,2) and len(cm_thresh[0]) > 1 else 0\n",
    "        fn_thresh = cm_thresh[1,0] if cm_thresh.shape == (2,2) and len(cm_thresh) > 1 else 0\n",
    "        \n",
    "        threshold_analysis_results.append({\n",
    "            'Threshold': thresh_val, 'Precision': p, 'Recall': r, 'F1': f1, 'FP': fp_thresh, 'FN': fn_thresh\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(threshold_analysis_results)\n",
    "    print(\"\\nValidation Set Threshold Analysis Results:\")\n",
    "    print(results_df)\n",
    "    results_df.to_csv(RESULTS_DIR / \"autoencoder_threshold_analysis.csv\", index=False)\n",
    "    \n",
    "    if optimization_metric == 'f1' and 'F1' in results_df.columns and not results_df['F1'].empty:\n",
    "        best_row = results_df.loc[results_df['F1'].idxmax()]\n",
    "    elif optimization_metric == 'precision' and 'Precision' in results_df.columns and not results_df['Precision'].empty:\n",
    "        best_row = results_df.sort_values(by=['Precision', 'Recall'], ascending=[False, False]).iloc[0]\n",
    "    elif optimization_metric == 'recall' and 'Recall' in results_df.columns and not results_df['Recall'].empty:\n",
    "        best_row = results_df.sort_values(by=['Recall', 'Precision'], ascending=[False, False]).iloc[0]\n",
    "    elif 'F1' in results_df.columns and not results_df['F1'].empty: # Fallback to F1 if metric not found or empty\n",
    "        best_row = results_df.loc[results_df['F1'].idxmax()]\n",
    "    else: # Absolute fallback if F1 is also problematic\n",
    "        print(\"Warning: Could not determine best threshold based on optimization metric. Using median error as threshold.\")\n",
    "        best_row = pd.Series({'Threshold': np.median(val_reconstruction_errors), 'Precision': np.nan, 'Recall': np.nan, 'F1': np.nan})\n",
    "\n",
    "\n",
    "    chosen_threshold = best_row['Threshold']\n",
    "    print(f\"\\nChosen threshold based on '{optimization_metric}' on validation set: {chosen_threshold:.4f} (P: {best_row['Precision']:.3f}, R: {best_row['Recall']:.3f}, F1: {best_row['F1']:.3f})\")\n",
    "    \n",
    "    print(\"\\nCalculating reconstruction errors on test set...\")\n",
    "    test_predictions = model.predict(X_test, batch_size=AUTOENCODER_PARAMS.get('batch_size',256), verbose=0)\n",
    "    test_reconstruction_errors = np.mean(np.square(X_test - test_predictions), axis=(1, 2))\n",
    "    \n",
    "    test_anomalies_predicted = (test_reconstruction_errors > chosen_threshold).astype(int)\n",
    "    \n",
    "    return test_anomalies_predicted, test_reconstruction_errors, chosen_threshold\n",
    "\n",
    "\n",
    "def evaluate_anomaly_detection_ad(y_true_labels: np.ndarray, \n",
    "                               y_pred_labels: np.ndarray,\n",
    "                               anomaly_scores_for_roc_pr: np.ndarray,\n",
    "                               model_name: str,\n",
    "                               chosen_threshold_for_plotting: float) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate anomaly detection model performance.\"\"\"\n",
    "    y_true_binary = (np.any(y_true_labels > 0, axis=1)).astype(int) if len(y_true_labels.shape) > 1 and y_true_labels.shape[1] > 1 else (y_true_labels > 0.5).astype(int)\n",
    "    \n",
    "    if len(y_pred_labels) != len(y_true_binary):\n",
    "        raise ValueError(f\"Length mismatch in evaluate_anomaly_detection_ad: y_true_binary ({len(y_true_binary)}) vs y_pred_labels ({len(y_pred_labels)})\")\n",
    "    if len(anomaly_scores_for_roc_pr) != len(y_true_binary):\n",
    "         raise ValueError(f\"Length mismatch: y_true_binary ({len(y_true_binary)}) vs anomaly_scores_for_roc_pr ({len(anomaly_scores_for_roc_pr)})\")\n",
    "\n",
    "    precision = precision_score(y_true_binary, y_pred_labels, zero_division=0)\n",
    "    recall = recall_score(y_true_binary, y_pred_labels, zero_division=0)\n",
    "    f1 = f1_score(y_true_binary, y_pred_labels, zero_division=0)\n",
    "    \n",
    "    roc_auc_val = np.nan\n",
    "    avg_precision_val = np.nan\n",
    "\n",
    "    if len(np.unique(y_true_binary)) > 1:\n",
    "        try:\n",
    "            roc_auc_val = roc_auc_score(y_true_binary, anomaly_scores_for_roc_pr)\n",
    "            avg_precision_val = average_precision_score(y_true_binary, anomaly_scores_for_roc_pr)\n",
    "        except ValueError as e:\n",
    "            print(f\"Warning for {model_name}: Could not calculate AUC/AP: {e}\")\n",
    "    else:\n",
    "        print(f\"Warning for {model_name}: Only one class present in true labels. AUC/AP cannot be computed meaningfully.\")\n",
    "\n",
    "    print(f\"\\n{model_name} Classification Report (Test Set):\")\n",
    "    print(classification_report(y_true_binary, y_pred_labels, zero_division=0, target_names=['Normal', 'Anomaly']))\n",
    "    \n",
    "    \n",
    "    plot_confusion_matrix(y_true_binary, y_pred_labels, model_name) \n",
    "    if not np.isnan(roc_auc_val):\n",
    "        plot_roc_curve(y_true_binary, anomaly_scores_for_roc_pr, model_name) \n",
    "    if not np.isnan(avg_precision_val):\n",
    "        plot_precision_recall_curve(y_true_binary, anomaly_scores_for_roc_pr, model_name)\n",
    "    \n",
    "    plot_anomaly_scores_distribution(y_true_binary, anomaly_scores_for_roc_pr, model_name)\n",
    "\n",
    "    metrics_dict = {\n",
    "        'precision': float(precision), 'recall': float(recall), 'f1_score': float(f1),\n",
    "        'auc_score': float(roc_auc_val) if not np.isnan(roc_auc_val) else None, \n",
    "        'average_precision': float(avg_precision_val) if not np.isnan(avg_precision_val) else None\n",
    "    }\n",
    "    \n",
    "    model_metrics_df = pd.DataFrame([metrics_dict])\n",
    "    model_metrics_df.insert(0, 'Model', model_name)\n",
    "    \n",
    "    main_metrics_file = RESULTS_DIR / 'all_anomaly_metrics.csv' \n",
    "    if main_metrics_file.exists():\n",
    "        existing_metrics_df = pd.read_csv(main_metrics_file)\n",
    "        existing_metrics_df = existing_metrics_df[existing_metrics_df['Model'] != model_name] \n",
    "        combined_metrics_df = pd.concat([existing_metrics_df, model_metrics_df], ignore_index=True)\n",
    "    else:\n",
    "        combined_metrics_df = model_metrics_df\n",
    "    combined_metrics_df.to_csv(main_metrics_file, index=False)\n",
    "\n",
    "    with open(RESULTS_DIR / f'{model_name.lower()}_metrics_ad.json', 'w') as f: \n",
    "        json.dump(metrics_dict, f, indent=4)\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "def run_anomaly_detection():\n",
    "    \"\"\"Run anomaly detection with Autoencoder.\"\"\"\n",
    "    print(\"\\nStarting Autoencoder Anomaly Detection Phase...\")\n",
    "    start_time_main = time.time()\n",
    "\n",
    "    X_train_ad = np.load(DATA_DIR / 'X_train_ad.npy') \n",
    "    X_val_ad_mixed = np.load(DATA_DIR / 'X_val_ad.npy') \n",
    "    y_val_ad_labels = np.load(DATA_DIR / 'y_val_ad_labels.npy') \n",
    "    X_test_ad_mixed = np.load(DATA_DIR / 'X_test_ad.npy') \n",
    "    y_test_ad_labels = np.load(DATA_DIR / 'y_test_ad_labels.npy') \n",
    "\n",
    "    print(\"\\nData shapes for Autoencoder:\")\n",
    "    print(f\"X_train_ad (normal only): {X_train_ad.shape}\")\n",
    "    print(f\"X_val_ad (mixed): {X_val_ad_mixed.shape}, y_val_ad_labels: {y_val_ad_labels.shape}\")\n",
    "    print(f\"X_test_ad (mixed): {X_test_ad_mixed.shape}, y_test_ad_labels: {y_test_ad_labels.shape}\")\n",
    "\n",
    "    print(\"\\nTraining Autoencoder...\")\n",
    "    input_shape_ae = (X_train_ad.shape[1], X_train_ad.shape[2])\n",
    "    autoencoder = create_autoencoder(input_shape=input_shape_ae)\n",
    "    autoencoder_results = train_autoencoder(autoencoder, X_train_ad, X_val_ad_mixed) \n",
    "    \n",
    "    print(\"\\nDetecting anomalies with Autoencoder...\")\n",
    "    predicted_anomalies_test, test_anomaly_scores, chosen_threshold_from_val = detect_anomalies_autoencoder(\n",
    "        autoencoder_results['model'],\n",
    "        X_test_ad_mixed,\n",
    "        X_val_ad_mixed, \n",
    "        y_val_ad_labels, \n",
    "        threshold_percentiles_list=AUTOENCODER_PARAMS['threshold_analysis_percentiles'],\n",
    "        optimization_metric=AUTOENCODER_PARAMS['threshold_optimization_metric']\n",
    "    )\n",
    "    \n",
    "    print(\"\\nEvaluating Autoencoder on Test Set...\")\n",
    "    autoencoder_test_metrics = evaluate_anomaly_detection_ad(\n",
    "        y_test_ad_labels, predicted_anomalies_test, test_anomaly_scores, \"Autoencoder\", chosen_threshold_from_val\n",
    "    )\n",
    "    \n",
    "    # Assuming save_model is globally available (from Cell 2 or imported)\n",
    "    save_model(autoencoder_results['model'], 'autoencoder_anomaly_model')\n",
    "    \n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        print(f\"\\nTotal Autoencoder anomaly detection phase execution time: {time.time() - start_time_main:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\nAutoencoder Anomaly Detection phase completed!\")\n",
    "    return autoencoder_test_metrics\n",
    "\n",
    "\n",
    "autoencoder_evaluation_metrics = run_anomaly_detection()\n",
    "print(\"\\nAutoencoder trained and evaluated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phase 3: Final Model Evaluation and Comparison\n",
    "\n",
    "This final phase consolidates the results from the forecasting and anomaly detection models, provides comparative visualizations, and generates a summary report.\n",
    "\n",
    "**What this cell does:**\n",
    "1.  **Define Local Plotting Utilities (if needed, or rely on Cell 2):**\n",
    "    *   `plot_confusion_matrix_eval()` and `plot_roc_curve_eval()`: These are defined locally within this cell (as per the provided script) to potentially customize evaluation-phase plots or avoid conflicts if utility functions in Cell 2 were modified. In a fully streamlined notebook, these would ideally be consolidated with the utilities in Cell 2.\n",
    "2.  **Define Comparison and Reporting Functions:**\n",
    "    *   `compare_forecasting_metrics_dfs()`: Loads forecasting metrics from JSON files (one per model, e.g., `lstm_metrics.json`, `tcn_metrics.json`), creates a comparison DataFrame, plots a bar chart of key metrics (MSE, MAE, RMSE, prioritizing denormalized versions), and saves the plot.\n",
    "    *   `compare_anomaly_detection_metrics_dfs()`: Loads anomaly detection metrics from JSON files (e.g., `autoencoder_anomaly_model_metrics_ad.json`), creates a comparison DataFrame, plots a bar chart (Precision, Recall, F1, AUC, Avg. Precision), and saves it. *Alternatively, it's set up to load from `all_anomaly_metrics.csv` if available, which is a more consolidated source.*\n",
    "    *   `load_all_data_for_evaluation()`: Loads necessary test data arrays (`X_test_ad`, `y_test_ad_labels`), forecasting predictions, and recalculates/loads Autoencoder anomaly scores and predicted labels for the test set using the previously determined optimal threshold. This ensures all data needed for final evaluation plots and summaries is available.\n",
    "    *   `generate_final_summary_report()`: Takes the comparison DataFrames from forecasting and anomaly detection. It identifies the best performing model for each task based on a primary metric (e.g., MAE for forecasting, F1-score for anomaly detection). It also compiles key insights and general recommendations for improvement.\n",
    "    *   `save_evaluation_results()`: (This function was in the original `evaluate_models.py` but its functionality is largely covered by individual saving actions in other functions and the `generate_final_summary_report` saving its own output). For this notebook cell, the main output saving is the `final_evaluation_summary_report.json`.\n",
    "3.  **`run_evaluation_phase()` Function:**\n",
    "    *   Orchestrates the final evaluation.\n",
    "    *   Calls `load_all_data_for_evaluation()` to get test data and model outputs.\n",
    "    *   Calls `compare_forecasting_metrics_dfs()` to compare LSTM and TCN.\n",
    "    *   Calls `compare_anomaly_detection_metrics_dfs()` to display (and potentially re-plot) anomaly detection model metrics (primarily Autoencoder in this setup).\n",
    "    *   Calls `generate_final_summary_report()` to create a JSON summary.\n",
    "    *   Prints key findings and recommendations from the summary report.\n",
    "4.  **Execution:** Calls `run_evaluation_phase()` to perform all comparison and reporting steps.\n",
    "\n",
    "**Why these steps are performed:**\n",
    "*   **Consolidated View:** To bring together results from different modeling phases for a holistic understanding of the project's outcomes.\n",
    "*   **Model Benchmarking:** To quantitatively and visually compare the performance of different approaches (LSTM vs. TCN for forecasting; and potentially different anomaly detectors if more were added).\n",
    "*   **Actionable Insights:** To derive conclusions about which models perform best for specific tasks and to identify areas for future improvement.\n",
    "*   **Reporting:** To generate a structured summary (JSON file) that encapsulates the main findings, suitable for documentation or sharing.\n",
    "\n",
    "**Expected Output:**\n",
    "*   Print statements indicating the start of the evaluation phase and data loading.\n",
    "*   If forecasting metric files exist:\n",
    "    *   A Pandas DataFrame comparing LSTM and TCN forecasting metrics.\n",
    "    *   A saved plot `final_forecasting_models_comparison.png`.\n",
    "*   If anomaly detection metric files/CSVs exist:\n",
    "    *   A Pandas DataFrame comparing anomaly detection model(s) metrics.\n",
    "    *   A saved plot `final_anomaly_detection_models_comparison.png`.\n",
    "*   Key insights and recommendations printed from the generated summary report.\n",
    "*   A `final_evaluation_summary_report.json` file saved in the `RESULTS_DIR`.\n",
    "*   A final message indicating the completion of the evaluation phase.\n",
    "*   The cell will return the `summary_report` dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase3 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model Evaluation and Comparison\n",
    "\n",
    "def plot_confusion_matrix_eval(y_true: np.ndarray, \n",
    "                         y_pred: np.ndarray,\n",
    "                         model_name: str):\n",
    "    \"\"\"Plot confusion matrix for anomaly detection results.\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'eval_confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curve_eval(y_true: np.ndarray, \n",
    "                  y_score: np.ndarray,\n",
    "                  model_name: str):\n",
    "    \"\"\"Plot ROC curve for anomaly detection results.\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / f'eval_roc_curve_{model_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def compare_forecasting_metrics_dfs(forecasting_metrics_files: Dict[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Load and compare forecasting model metrics from JSON files.\"\"\"\n",
    "    all_metrics_list = []\n",
    "    for model_name, file_path in forecasting_metrics_files.items():\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            metrics['model'] = model_name\n",
    "            all_metrics_list.append(metrics)\n",
    "        else:\n",
    "            print(f\"Warning: Metrics file not found for {model_name} at {file_path}\")\n",
    "    \n",
    "    if not all_metrics_list:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    metrics_df = pd.DataFrame(all_metrics_list)\n",
    "    metrics_df = metrics_df.set_index('model')\n",
    "    \n",
    "    \n",
    "    plot_cols = [col for col in ['mae_denorm', 'rmse_denorm', 'mse_denorm'] if col in metrics_df.columns]\n",
    "    if not plot_cols:\n",
    "        plot_cols = [col for col in ['mae_scaled', 'rmse_scaled', 'mse_scaled'] if col in metrics_df.columns]\n",
    "\n",
    "    if plot_cols:\n",
    "        metrics_df[plot_cols].plot(kind='bar', figsize=(12, 7))\n",
    "        plt.title('Forecasting Models Comparison')\n",
    "        plt.ylabel('Metric Value')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS_DIR / 'final_forecasting_models_comparison.png')\n",
    "        plt.close()\n",
    "    else:\n",
    "        print(\"No suitable metrics found for plotting forecasting comparison.\")\n",
    "        \n",
    "    return metrics_df\n",
    "\n",
    "\n",
    "def compare_anomaly_detection_metrics_dfs(anomaly_metrics_files: Dict[str, Path]) -> pd.DataFrame:\n",
    "    \"\"\"Load and compare anomaly detection model metrics from JSON files.\"\"\"\n",
    "    all_metrics_list = []\n",
    "    for model_name, file_path in anomaly_metrics_files.items():\n",
    "        if file_path.exists():\n",
    "            with open(file_path, 'r') as f:\n",
    "                metrics = json.load(f)\n",
    "            metrics['model'] = model_name\n",
    "            all_metrics_list.append(metrics)\n",
    "        else:\n",
    "            print(f\"Warning: Metrics file not found for {model_name} at {file_path}\")\n",
    "\n",
    "    if not all_metrics_list:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    metrics_df = pd.DataFrame(all_metrics_list)\n",
    "    metrics_df = metrics_df.set_index('model')\n",
    "    \n",
    "    plot_cols = ['precision', 'recall', 'f1_score', 'auc_score', 'average_precision']\n",
    "    metrics_df[plot_cols].plot(kind='bar', figsize=(12, 7))\n",
    "    plt.title('Anomaly Detection Models Comparison')\n",
    "    plt.ylabel('Metric Value')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 1.05) # Metrics are typically between 0 and 1\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS_DIR / 'final_anomaly_detection_models_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "def load_all_data_for_evaluation() -> Dict[str, Any]:\n",
    "    \"\"\"Load all necessary data for the final evaluation phase.\"\"\"\n",
    "    data = {}\n",
    "    try:\n",
    "        print(\"Loading test data for final evaluation...\")\n",
    "        data['X_test_ad'] = np.load(DATA_DIR / 'X_test_ad.npy')\n",
    "        data['y_test_ad_labels'] = np.load(DATA_DIR / 'y_test_ad_labels.npy')\n",
    "        # For forecasting evaluation, if needed, load y_test_forecast\n",
    "        data['y_test_forecast_pred_features'] = np.load(DATA_DIR / 'y_test_forecast.npy')[:, [FEATURE_COLUMNS.index(f) for f in PREDICTION_FEATURES]]\n",
    "\n",
    "        if (DATA_DIR / 'test_timestamps_for_sequences.npy').exists():\n",
    "            data['test_timestamps_for_sequences'] = pd.to_datetime(np.load(DATA_DIR / 'test_timestamps_for_sequences.npy'))\n",
    "        else:\n",
    "            data['test_timestamps_for_sequences'] = None\n",
    "            print(\"Warning: test_timestamps_for_sequences.npy not found.\")\n",
    "        print(\"Test data loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading base test data: {e}\")\n",
    "        \n",
    "        return {'error': 'Failed to load essential test data.'}\n",
    "\n",
    "    # Load model predictions and scores\n",
    "    try:\n",
    "        print(\"Loading model predictions and scores...\")\n",
    "        data['lstm_predictions_denorm'] = np.load(RESULTS_DIR / 'lstm_test_predictions_denorm.npy')\n",
    "        data['tcn_predictions_denorm'] = np.load(RESULTS_DIR / 'tcn_test_predictions_denorm.npy')\n",
    "        \n",
    "        \n",
    "        autoencoder_threshold_df = pd.read_csv(RESULTS_DIR / \"autoencoder_threshold_analysis.csv\")\n",
    "        best_f1_row = autoencoder_threshold_df.loc[autoencoder_threshold_df[AUTOENCODER_PARAMS['threshold_optimization_metric']].idxmax()]\n",
    "        data['autoencoder_chosen_threshold'] = best_f1_row['Threshold']\n",
    "        \n",
    "        \n",
    "        ae_model = load_model('autoencoder_anomaly_model') \n",
    "        ae_test_predictions = ae_model.predict(data['X_test_ad'])\n",
    "        data['autoencoder_test_scores'] = np.mean(np.square(data['X_test_ad'] - ae_test_predictions), axis=(1,2))\n",
    "        data['autoencoder_test_predicted_labels'] = (data['autoencoder_test_scores'] > data['autoencoder_chosen_threshold']).astype(int)\n",
    "        print(\"Model predictions and scores loaded/recalculated.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model outputs: {e}\")\n",
    "        data['load_output_error'] = str(e)\n",
    "        \n",
    "    return data\n",
    "\n",
    "def generate_final_summary_report(forecast_comparison_df: pd.DataFrame, anomaly_comparison_df: pd.DataFrame) -> Dict[str, Any]:\n",
    "    \"\"\"Generate a comprehensive summary report of all model performances.\"\"\"\n",
    "    report = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'forecasting_summary': None,\n",
    "        'anomaly_detection_summary': None,\n",
    "        'overall_best_forecaster': None,\n",
    "        'overall_best_anomaly_detector': None,\n",
    "        'key_insights': [],\n",
    "        'recommendations_for_improvement': []\n",
    "    }\n",
    "\n",
    "    if not forecast_comparison_df.empty:\n",
    "        report['forecasting_summary'] = forecast_comparison_df.to_dict()\n",
    "        # Determine best forecaster based on a primary metric, e.g., MAE (lower is better)\n",
    "        primary_forecast_metric = 'mae_denorm' if 'mae_denorm' in forecast_comparison_df.columns else 'mae_scaled'\n",
    "        if primary_forecast_metric in forecast_comparison_df.columns:\n",
    "            report['overall_best_forecaster'] = forecast_comparison_df[primary_forecast_metric].idxmin()\n",
    "            report['key_insights'].append(f\"Best forecasting model ({primary_forecast_metric}): {report['overall_best_forecaster']}\")\n",
    "        else:\n",
    "            report['key_insights'].append(\"Could not determine best forecasting model due to missing MAE metrics.\")\n",
    "\n",
    "\n",
    "    if not anomaly_comparison_df.empty:\n",
    "        report['anomaly_detection_summary'] = anomaly_comparison_df.to_dict()\n",
    "        # Determine best anomaly detector based on F1-score (higher is better)\n",
    "        if 'F1 Score' in anomaly_comparison_df.columns: # Ensure column name matches exactly\n",
    "            report['overall_best_anomaly_detector'] = anomaly_comparison_df['F1 Score'].idxmax()\n",
    "            report['key_insights'].append(f\"Best anomaly detection model (F1 Score): {report['overall_best_anomaly_detector']}\")\n",
    "\n",
    "            \n",
    "            best_detector_stats = anomaly_comparison_df.loc[report['overall_best_anomaly_detector']]\n",
    "            if best_detector_stats['Recall'] < 0.5:\n",
    "                report['key_insights'].append(f\"{report['overall_best_anomaly_detector']} has low recall ({best_detector_stats['Recall']:.3f}), missing many anomalies.\")\n",
    "            if best_detector_stats['Precision'] < 0.5:\n",
    "                report['key_insights'].append(f\"{report['overall_best_anomaly_detector']} has low precision ({best_detector_stats['Precision']:.3f}), many false alarms.\")\n",
    "        else:\n",
    "            report['key_insights'].append(\"Could not determine best anomaly detection model due to missing F1 Score metric.\")\n",
    "\n",
    "\n",
    "    report['recommendations_for_improvement'] = [\n",
    "        \"Further tune hyperparameters for all models.\",\n",
    "        \"Explore more diverse data augmentation techniques if real-world data is limited.\",\n",
    "        \"Investigate feature importance and potentially engineer new relevant features.\",\n",
    "        \"If Autoencoder recall remains low, explore different anomaly detection algorithms or ensemble methods.\"\n",
    "    ]\n",
    "    \n",
    "    return report\n",
    "\n",
    "def run_evaluation_phase():\n",
    "    \"\"\"Main function to run the final evaluation and comparison phase.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Starting Final Model Evaluation & Comparison Phase\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    start_time_main = time.time()\n",
    "\n",
    "    # 1. Load all necessary data and pre-computed results/models\n",
    "    loaded_eval_data = load_all_data_for_evaluation()\n",
    "    if 'error' in loaded_eval_data:\n",
    "        print(f\"Critical error during data loading: {loaded_eval_data['error']}. Aborting evaluation.\")\n",
    "        return\n",
    "\n",
    "    # 2. Consolidate and Compare Forecasting Metrics\n",
    "    print(\"\\n--- Comparing Forecasting Models ---\")\n",
    "    forecasting_metric_files = {\n",
    "        'LSTM': RESULTS_DIR / 'lstm_metrics.json',\n",
    "        'TCN': RESULTS_DIR / 'tcn_metrics.json'\n",
    "    }\n",
    "    forecasting_comparison_df = compare_forecasting_metrics_dfs(forecasting_metric_files)\n",
    "    if not forecasting_comparison_df.empty:\n",
    "        print(\"\\nForecasting Metrics Comparison:\")\n",
    "        print(forecasting_comparison_df)\n",
    "    else:\n",
    "        print(\"No forecasting metrics to compare.\")\n",
    "\n",
    "    # 3. Consolidate and Compare Anomaly Detection Metrics\n",
    "    print(\"\\n--- Comparing Anomaly Detection Models ---\")\n",
    "    \n",
    "    anomaly_metric_files = {\n",
    "        'Autoencoder': RESULTS_DIR / 'autoencoder_anomaly_model_metrics_ad.json' \n",
    "    }\n",
    "    \n",
    "    all_anomaly_metrics_path = RESULTS_DIR / 'all_anomaly_metrics.csv'\n",
    "    if all_anomaly_metrics_path.exists():\n",
    "        anomaly_comparison_df = pd.read_csv(all_anomaly_metrics_path).set_index('Model')\n",
    "        print(\"\\nAnomaly Detection Metrics Comparison (from all_anomaly_metrics.csv):\")\n",
    "        print(anomaly_comparison_df)\n",
    "        \n",
    "        if not anomaly_comparison_df.empty:\n",
    "            plot_cols_ad = ['Precision', 'Recall', 'F1 Score', 'AUC Score', 'Average Precision']\n",
    "            \n",
    "            plot_cols_ad_present = [col for col in plot_cols_ad if col in anomaly_comparison_df.columns]\n",
    "            if plot_cols_ad_present:\n",
    "                anomaly_comparison_df[plot_cols_ad_present].plot(kind='bar', figsize=(12, 7))\n",
    "                plt.title('Anomaly Detection Models Comparison (Final)')\n",
    "                plt.ylabel('Metric Value')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.ylim(0, 1.05) \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(RESULTS_DIR / 'final_anomaly_detection_models_comparison.png')\n",
    "                plt.close()\n",
    "\n",
    "    else:\n",
    "        print(f\"Warning: {all_anomaly_metrics_path} not found. Cannot generate full anomaly detection comparison.\")\n",
    "        anomaly_comparison_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    \n",
    "    print(\"\\n--- Generating Final Summary Report ---\")\n",
    "    final_report_data = generate_final_summary_report(forecasting_comparison_df, anomaly_comparison_df)\n",
    "    \n",
    "    \n",
    "    with open(RESULTS_DIR / 'final_evaluation_summary_report.json', 'w') as f:\n",
    "        json.dump(final_report_data, f, indent=4, default=str) \n",
    "    print(f\"Final summary report saved to {RESULTS_DIR / 'final_evaluation_summary_report.json'}\")\n",
    "\n",
    "    \n",
    "    print(\"\\nKey Insights from Final Report:\")\n",
    "    for insight in final_report_data.get('key_insights', []):\n",
    "        print(f\"- {insight}\")\n",
    "    \n",
    "    print(\"\\nRecommendations for Improvement:\")\n",
    "    for rec in final_report_data.get('recommendations_for_improvement', []):\n",
    "        print(f\"- {rec}\")\n",
    "\n",
    "    if PRINT_PARAMS['show_timing']:\n",
    "        print(f\"\\nTotal evaluation phase execution time: {time.time() - start_time_main:.2f} seconds\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Final Model Evaluation & Comparison Phase Completed!\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "run_evaluation_phase()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFwKeras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
